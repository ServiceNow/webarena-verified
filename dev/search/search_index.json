{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to WebArena-Verified","text":"<p>WebArena-Verified is the reproducible release of the WebArena benchmark: the original containerized sites remain intact, but every task, reference answer, and evaluator has been re-audited to eliminate brittle string matching and ambiguous success criteria. Deterministic, JSON-based scoring and network events-based checks let you measure web agents offline.</p> <p>Key Contributions:</p> <ul> <li>Fully audited benchmark: Every task, reference answer, and evaluator has been manually reviewed and corrected</li> <li>Offline evaluation: Evaluate agent runs without requiring live web environments using network trace replay</li> <li>Deterministic scoring: Removed LLM-as-a-judge evaluation and substring matching in favor of type-aware normalization and structural comparison</li> <li>WebArena-Verified Hard subset: A difficulty-prioritized 258-task subset for cost-effective evaluation</li> </ul> <p>The following quick start demonstrates these capabilities in practice. You'll bring the toolkit up, validate a single task end-to-end, and then branch into batch evaluation or custom integrations when you're ready.</p> <p>This quick start is divided into two parts:</p> <ul> <li>Part 1 (~5 minutes): Understand evaluation by evaluating a pre-run agent log</li> <li>Part 2 (~10 minutes): Run an agent and evaluate it</li> </ul>"},{"location":"#setup","title":"Setup","text":"<p>Clone the repository and install dependencies:</p> uvxDockeruvpip <p>Prerequisites: uv</p> <p>What is uvx?</p> <p><code>uvx</code> runs Python CLI tools in isolated, ephemeral environments without installation. It doesn't pollute your environment and automatically handles dependencies and cleanup.</p> <pre><code>git clone https://github.com/ServiceNow/webarena-verified.git\ncd webarena-verified\n</code></pre> <p>Verify the CLI is working:</p> <pre><code>uvx webarena-verified --help\n</code></pre> <p>Prerequisites: Docker</p> <pre><code>git clone https://github.com/ServiceNow/webarena-verified.git\ncd webarena-verified\ndocker pull am1n3e/webarena-verified:latest\n</code></pre> <p>Verify the CLI is working:</p> <pre><code>docker run --rm am1n3e/webarena-verified:latest --help\n</code></pre> <p>Prerequisites: Python 3.11+</p> <pre><code>git clone https://github.com/ServiceNow/webarena-verified.git\ncd webarena-verified\nuv venv\nsource .venv/bin/activate\nuv pip install \"webarena-verified[examples]\"\n</code></pre> <p>Verify the CLI is working:</p> <pre><code>webarena-verified --help\n</code></pre> <p>Prerequisites: Python 3.11+</p> <pre><code>git clone https://github.com/ServiceNow/webarena-verified.git\ncd webarena-verified\npython -m venv .venv\nsource .venv/bin/activate\npip install \"webarena-verified[examples]\"\n</code></pre> <p>Verify the CLI is working:</p> <pre><code>webarena-verified --help\n</code></pre> <p>Why clone the repository?</p> <p>This tutorial uses example files (pre-run agent logs, configs, and the human agent) from the <code>examples/</code> directory. If you're evaluating your own agents, you can simply <code>pip install webarena-verified</code> or use the Docker image which contains the self-contained evaluator - see the Usage Guide.</p>"},{"location":"#part-1-evaluate-a-pre-run-task","title":"Part 1: Evaluate a Pre-Run Task","text":"<p>Before running an agent, let's evaluate an existing agent log to understand how WebArena-Verified works. We'll use the following task that already has output in <code>examples/agent_logs/demo/108/</code>:</p> <pre><code>{\n  \"task_id\": 108,\n  \"intent\": \"Get the monthly count of successful orders 01/2023-05/2023\",\n  \"sites\": [\"shopping_admin\"]\n  ...\n}\n</code></pre> <p>New in WebArena-Verified: Offline Evaluation.</p> <p>Why This Matters:</p> <ul> <li>Evaluate agent runs without live web environments</li> <li>Reevaluate past runs at any time</li> <li>Compare different agents transparently with reproducible benchmarking</li> </ul>"},{"location":"#1-whats-in-a-task-log","title":"1. What's in a Task Log?","text":"<p>The task log contains two key artifacts:</p> <pre><code>examples/agent_logs/demo/108/\n\u251c\u2500\u2500 agent_response.json\n\u2514\u2500\u2500 network.har\n</code></pre>"},{"location":"#agent-response","title":"Agent Response","text":"<p>Agents are required to return a valid JSON like the following:</p> <pre><code>{\n  \"task_type\": \"RETRIEVE\",\n  \"status\": \"SUCCESS\",\n  \"retrieved_data\": [\n    { \"month\": \"Jan\", \"count\": 12 },\n    { \"month\": \"Feb\", \"count\": 7 },\n    { \"month\": \"March\", \"count\": 5 },\n    { \"month\": \"April\", \"count\": 9 },\n    { \"month\": \"May\", \"count\": 5 }\n  ],\n  \"error_details\": null\n}\n</code></pre> Field Descriptions <ul> <li><code>task_type</code> (required): Type of work performed - <code>RETRIEVE</code>, <code>MUTATE</code>, or <code>NAVIGATE</code></li> <li><code>status</code> (required): Task outcome - <code>SUCCESS</code> or error codes (<code>NOT_FOUND_ERROR</code>, <code>PERMISSION_DENIED_ERROR</code>, <code>DATA_VALIDATION_ERROR</code>, etc.)</li> <li><code>retrieved_data</code>: Array of items for <code>RETRIEVE</code> operations (otherwise null)</li> <li><code>error_details</code>: Null for <code>SUCCESS</code>, otherwise explains what failed and why</li> </ul> <p>New in WebArena-Verified: Structured Agent Response</p> <p>Why This Matters:</p> <ul> <li>Robust Evaluation: Modern LLMs rarely struggle with generating valid JSON, enabling more reliable evaluation with explicit fields:<ul> <li><code>task_type</code>: Requires agents to explicitly state what operation they performed, revealing whether they truly understood the task</li> <li><code>status</code>: Allows various error status codes instead of catch-all \"N/A\" responses for unachievable tasks</li> <li><code>retrieved_data</code>: Structured format reduces false negatives due to parsing issues</li> </ul> </li> <li> <p>Reduces False Positives: By validating both the operation type and the outcome, we ensure agents actually completed the intended task.</p> <p>Example: Navigation vs. Retrieval</p> <p>For a task that requires retrieving data, the agent misunderstands and only navigates. The agent can reach the correct page but never retrieve the data.</p> <ul> <li>Original WebArena: Pass \u2713 (only checked if agent reached the correct page)</li> <li>WebArena-Verified: Fail \u2717 (verifies page navigation and <code>task_type</code> matches <code>RETRIEVE</code>)</li> </ul> <p>(Or picture asking a coding agent to \"review this code\" and watching it start rewriting everything while you frantically mash Ctrl+C! \ud83d\ude31)</p> </li> </ul>"},{"location":"#network-trace","title":"Network Trace","text":"<p>Captures all network activity between the browser frontend and the backend in HAR (HTTP Archive) format - a standard format widely used for debugging and analyzing web traffic. This records what the agent actually did - including page navigations, data retrievals, and modifications. Each network event includes the URL, HTTP method, and response status used by the evaluator:</p> <pre><code>{\n  \"request\": {\n    \"method\": \"GET\",\n    \"url\": \"http://192.168.1.35:7780/admin/customer/index/\",\n    ...\n  },\n  \"response\": {\n    \"status\": 200,\n    ...\n  },\n  ...\n}\n</code></pre> <p>New in WebArena-Verified: Network Event Based Evaluation</p> <p>Why This Matters:</p> <ul> <li>Enables Offline Evaluation: Network traces can be evaluated without live web environments - this is the critical piece that makes reevaluation possible</li> <li>Avoids Brittle Locators: No reliance on DOM selectors or page structure - allows for easy website updates</li> <li>Single Evaluation Method: Works uniformly across all websites (GitLab, e-commerce, forums, etc.)</li> </ul> <p>See Network Event Based Evaluation for details.</p>"},{"location":"#2-run-the-evaluator","title":"2. Run the evaluator","text":"uvxDockerCLI <pre><code>uvx webarena-verified eval-tasks \\\n  --task-ids 108 \\\n  --output-dir examples/agent_logs/demo \\\n  --config examples/configs/config.demo.json\n</code></pre> <pre><code>docker run --rm \\\n  -v ./examples:/examples \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks \\\n    --task-ids 108 \\\n    --output-dir /examples/agent_logs/demo \\\n    --config /examples/configs/config.demo.json\n</code></pre> <pre><code>webarena-verified eval-tasks \\\n  --task-ids 108 \\\n  --output-dir examples/agent_logs/demo \\\n  --config examples/configs/config.demo.json\n</code></pre> Troubleshooting <ul> <li>If the <code>webarena-verified</code> command is not available, make sure you have activated the virtual environment correctly. See the Setup section.</li> </ul> <p>This creates an <code>eval_result.json</code> file in the task directory (<code>examples/agent_logs/demo/108/</code>).</p>"},{"location":"#3-examine-the-evaluation-result","title":"3. Examine the evaluation result","text":"<p>The evaluation result is a structured JSON document that shows:</p> <ul> <li>The overall task status and score - Did the agent pass or fail?</li> <li>Individual evaluator results - Each evaluator (e.g., AgentResponseEvaluator) reports its findings</li> <li>Raw and normalized values - We show both <code>actual</code> (raw agent output) and <code>actual_normalized</code> (after type-aware normalization) to help you catch normalization issues and understand how values are being compared</li> <li>Reproducibility checksums - We track evaluation code and task dataset checksums to ensure consistent, reproducible evaluations across different runs and environments</li> </ul> <p>The annotated JSON below explains each field. Click the + markers to expand explanations:</p> <pre><code>{\n  \"task_id\": 108,\n  \"intent_template_id\": 270,\n  \"sites\": [\n    \"shopping_admin\"\n  ],\n  \"task_revision\": 2, // (1)!\n  \"status\": \"success\", // (2)!\n  \"score\": 1.0, // (3)!\n  \"evaluators_results\": [ // (4)!\n    {\n      \"evaluator_name\": \"AgentResponseEvaluator\", // (5)!\n      \"status\": \"success\",\n      \"score\": 1.0,\n      \"actual\": { // (6)!\n        \"task_type\": \"RETRIEVE\",\n        \"status\": \"SUCCESS\",\n        \"retrieved_data\": [\n          { \"month\": \"Jan\", \"count\": 12 },\n          { \"month\": \"Feb\", \"count\": 7 },\n          { \"month\": \"March\", \"count\": 5 },\n          { \"month\": \"April\", \"count\": 9 },\n          { \"month\": \"May\", \"count\": 5 }\n        ],\n        \"error_details\": null\n      },\n      \"actual_normalized\": { // (7)!\n        \"task_type\": \"retrieve\",\n        \"status\": \"success\",\n        \"retrieved_data\": [\n          { \"month\": \"january\", \"count\": 12.0 },\n          { \"month\": \"february\", \"count\": 7.0 },\n          { \"month\": \"march\", \"count\": 5.0 },\n          { \"month\": \"april\", \"count\": 9.0 },\n          { \"month\": \"may\", \"count\": 5.0 }\n        ]\n      },\n      \"expected\": { // (8)!\n        \"task_type\": \"retrieve\",\n        \"status\": \"success\",\n        \"retrieved_data\": [\n          { \"month\": \"january\", \"count\": 12.0 },\n          { \"month\": \"february\", \"count\": 7.0 },\n          { \"month\": \"march\", \"count\": 5.0 },\n          { \"month\": \"april\", \"count\": 9.0 },\n          { \"month\": \"may\", \"count\": 5.0 }\n        ]\n      },\n      \"assertions\": null, // (9)!\n      \"error_msg\": null // (10)!\n    }\n  ],\n  \"error_msg\": null,\n  \"webarena_verified_version\": \"2.0.0\", // (11)!\n  \"webarena_verified_evaluator_checksum\": \"a9e6da7e172f7ba62e388d445ccc974cc5df02529b833738051c54879319d4f8\", // (12)!\n  \"webarena_verified_data_checksum\": \"33048e32d7349835e3ea656348e59ba4ca43d2068cce24d3772134e402ef8f4b\" // (13)!\n}\n</code></pre> <ol> <li>Task revision number - incremented when task definition changes</li> <li>Overall evaluation status - <code>success</code> when all evaluators pass</li> <li>Overall score - 1.0 = complete success, 0.0 = failure</li> <li>Results from each evaluator that ran on this task</li> <li>Name of the evaluator - <code>AgentResponseEvaluator</code> validates structured agent responses</li> <li>Raw agent response before normalization - note mixed month formats (\"Jan\", \"Feb\", \"March\")</li> <li>Agent response after type-aware normalization - all months converted to lowercase (\"january\", \"february\", \"march\"). Notice how <code>actual_normalized</code> matches <code>expected</code> even though raw formats were mixed.</li> <li>Expected values from task definition - what the agent should return after normalization</li> <li>List of assertion failures - <code>null</code> means all checks passed</li> <li>Error message when the evaluation system itself encounters an error (not agent failures). When <code>error_msg</code> is not null, <code>status</code> is <code>ERROR</code>.</li> <li>WebArena-Verified version used for this evaluation</li> <li>Checksum of evaluator code - ensures evaluation logic hasn't changed</li> <li>Checksum of task dataset - ensures task definitions are consistent</li> </ol> <p>New in WebArena-Verified: Type-Aware Normalization</p> <p>Why This Matters:</p> <ul> <li>Handles Common Data Types: Automatically normalizes dates, currency, URLs, coordinates, and more without requiring LLM-based evaluation</li> <li>Format-Agnostic Comparison: In this example, month names are normalized regardless of format (\"Jan\" vs \"January\" vs \"january\"), ensuring reliable comparison</li> <li>Deterministic &amp; Cost-Effective: Eliminates the unpredictability and cost of LLM evaluators</li> </ul>"},{"location":"#part-2-run-and-evaluate-an-agent","title":"Part 2: Run and Evaluate an Agent","text":"<p>Now that you understand evaluation, let's run an agent and evaluate it. We'll complete the following task:</p> <pre><code>{\n  \"task_id\": 44,\n  \"intent\": \"Open my todos page\",\n  \"sites\": [\"gitlab\"]\n  ...\n}\n</code></pre> <p>We'll use a special \"human agent\" that opens a browser and hands control to you to complete this simple navigation task (requires clicking on a single menu item).</p> <p>Why not use a real AI agent implementation?</p> <p>The goal of this exercise is to walk through how to use the benchmark in a straightforward way, without additional complexity. By stepping through the process manually, you'll understand exactly what agents need to produce and how evaluation works.</p>"},{"location":"#0-install-playwright","title":"0. Install Playwright","text":"<p>The example agents use Playwright for browser automation. Install the Chromium browser:</p> uvxDockeruv / pip <p>Playwright runs locally</p> <p><code>uvx</code> is used for evaluation only. To run the human agent example, you need Playwright installed locally.</p> <pre><code>uv venv\nsource .venv/bin/activate\nuv pip install \"webarena-verified[examples]\"\nplaywright install chromium\n</code></pre> <p>Playwright runs locally</p> <p>The Docker image is used for evaluation only. To run the human agent example, you need Playwright installed locally.</p> <pre><code>uv venv\nsource .venv/bin/activate\nuv pip install \"webarena-verified[examples]\"\nplaywright install chromium\n</code></pre> <pre><code>playwright install chromium\n</code></pre>"},{"location":"#1-setup-gitlab-environment","title":"1. Setup GitLab Environment","text":"<p>First, you need a GitLab instance to work with. Choose one option:</p> Demo GitLabBring Your Own What is the Demo GitLab? <p>This is a lightweight, bare-bones GitLab Docker image instead of the full 100 GB+ GitLab instance from the original WebArena. For simple navigation tasks like \"Check out my todos\", this smaller image is perfectly sufficient and much faster to download and run on your laptop! However, this task is not part of our hard subset since it only requires basic navigation.</p> <p>Start the demo GitLab instance using Docker:</p> <pre><code>uv run invoke -r examples demo-gitlab-start\n</code></pre> <p>The GitLab instance takes 2-3 minutes to fully boot up. Wait until the command completes and shows the container status as 'running' before proceeding.</p> <p>Change the default port (8012)</p> <p>To use a different port, add the <code>--port</code> flag: <pre><code>uv run invoke -r examples demo-gitlab-start --port=8080\n</code></pre> Then update <code>examples/configs/config.demo.json</code> to match your port.</p> <p>We'll use <code>examples/configs/config.demo.json</code></p> <pre><code>{\n  \"environments\": {\n    \"__GITLAB__\": {\n      \"urls\": [\"http://localhost:8012\"],\n      \"credentials\": {\n        \"username\": \"root\",\n        \"password\": \"demopass\"\n      }\n    }\n  }\n}\n</code></pre> <p>If you have your own GitLab instance running (from the original webarena setup), update <code>examples/configs/config.demo.json</code> with your GitLab URL and credentials:</p> <pre><code>{\n  \"environments\": {\n    \"__GITLAB__\": {\n      \"urls\": [\"http://your-gitlab-url[:port]\"],\n      \"credentials\": {\n        \"username\": \"your-username\",\n        \"password\": \"your-password\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"#2-export-task-data","title":"2. Export Task Data","text":"<p>Export the task information that the agent needs:</p> uvxDockerCLI <pre><code>uvx webarena-verified agent-input-get \\\n  --task-ids 44 \\\n  --config examples/configs/config.demo.json \\\n  --output output/tasks.demo.json\n</code></pre> <pre><code>docker run --rm \\\n  -v ./examples:/examples \\\n  -v ./output:/output \\\n  am1n3e/webarena-verified:latest \\\n  agent-input-get \\\n    --task-ids 44 \\\n    --config /examples/configs/config.demo.json \\\n    --output /output/tasks.demo.json\n</code></pre> <pre><code>webarena-verified agent-input-get \\\n  --task-ids 44 \\\n  --config examples/configs/config.demo.json \\\n  --output output/tasks.demo.json\n</code></pre> <p>This exports only the fields that the agent needs to perform the task (<code>intent</code>, <code>start_urls</code>) and the IDs (<code>task_id</code>, <code>intent_template_id</code>, and <code>sites</code>). Since the <code>--config</code> argument is provided, URL templates like <code>__GITLAB__</code> are rendered to actual URLs (e.g., <code>http://localhost:8012</code>).</p> <p>New in WebArena-Verified: Agent runner does not depend on benchmark dependencies</p> <p>Why This Matters:</p> <ul> <li>Language &amp; Framework Freedom: Your agent implementation can use any programming language (Python, JavaScript, Go, etc.) or framework - no dependency on the benchmark's libraries</li> <li>Independent Versioning: Use any version of Playwright, Selenium, or other browser automation tools without conflicts with the benchmark</li> <li>Lightweight Integration: Agents only need to read JSON task files and produce standard output formats (JSON response + HAR trace)</li> <li>Alternative Approach: While we use <code>agent-input-get</code> CLI here to export tasks, you can also call WebArena-Verified's Python API directly within your agent code if you prefer programmatic access</li> </ul>"},{"location":"#3-your-turn-complete-the-task","title":"3. Your Turn: Complete the Task","text":"<p>Now let's run the human agent for Task ID 44 (from <code>output/tasks.demo.json</code> we generated earlier)</p> <pre><code>{\n  \"sites\": [\"gitlab\"],\n  \"task_id\": 44,\n  \"intent_template_id\": 303,\n  \"start_urls\": [\"http://localhost:8012\"],\n  \"intent\": \"Open my todos page\"\n}\n</code></pre> <pre><code>uv run python examples/agents/human/agent.py \\\n  --tasks-file output/tasks.demo.json \\\n  --task_id 44 \\\n  --task_output_dir output/demo-run/44 \\\n  --config examples/configs/config.demo.json\n</code></pre> <p>What happens next:</p> <ol> <li>The agent script opens a browser window and navigates to GitLab (login is handled automatically)</li> <li>Now it's your turn! Navigate to the todos page by clicking \"To-Do List\" in the left sidebar, then close the browser window</li> <li>The agent will prompt you in the terminal to generate the agent response saved to <code>agent_response.json</code></li> <li>The agent writes its response and network event logs to <code>output/demo-run/44/agent_response.json</code> and <code>output/demo-run/44/network.har</code></li> </ol> Example: Agent Response Questionnaire Output <pre><code>==============================================================\nBrowser closed. Generating the agent response questionnaire...\n==============================================================\n\n------------------------------------------------------------\nSelect the performed operation:\n\n1. RETRIEVE\n2. MUTATE\n3. NAVIGATE\n\nEnter choice number &gt; 3\n\n------------------------------------------------------------\nSelect the task status:\n\n1. SUCCESS\n2. ACTION_NOT_ALLOWED_ERROR\n3. PERMISSION_DENIED_ERROR\n4. NOT_FOUND_ERROR\n5. DATA_VALIDATION_ERROR\n6. UNKNOWN_ERROR\n\nEnter choice number &gt; 1\n\n\n------------------------------------------------------------\nProposed agent response:\n{\n  \"task_type\": \"NAVIGATE\",\n  \"status\": \"SUCCESS\",\n  \"retrieved_data\": null,\n  \"error_details\": null\n}\n------------------------------------------------------------\nConfirm and save this response?\n  1. Yes\n  2. No\n&gt; 1\n</code></pre>"},{"location":"#4-evaluate-your-run","title":"4. Evaluate Your Run","text":"<p>Now let's evaluate your performance:</p> uvxDockerCLI <pre><code>uvx webarena-verified eval-tasks \\\n  --config examples/configs/config.demo.json \\\n  --task-ids 44 \\\n  --output-dir output/demo-run\n</code></pre> <pre><code>docker run --rm \\\n  -v ./examples:/examples \\\n  -v ./output:/output \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks \\\n    --config /examples/configs/config.demo.json \\\n    --task-ids 44 \\\n    --output-dir /output/demo-run\n</code></pre> <pre><code>webarena-verified eval-tasks \\\n  --config examples/configs/config.demo.json \\\n  --task-ids 44 \\\n  --output-dir output/demo-run\n</code></pre> <p>This creates <code>output/demo-run/44/eval_result.json</code> with your evaluation results.</p>"},{"location":"#5-review-the-results","title":"5. Review the Results","text":"<p>Check <code>output/demo-run/44/eval_result.json</code> - it will have the same structure as Part 1.</p> <p>What got evaluated:</p> <ul> <li>AgentResponseEvaluator: Validated your response structure (<code>task_type</code>, <code>status</code>, etc.)</li> <li>NetworkEventEvaluator: Checked that you navigated to the correct URL (<code>/dashboard/todos</code>)</li> </ul> <p>If you successfully navigated to the todos page and reported <code>task_type: \"NAVIGATE\"</code> with <code>status: \"SUCCESS\"</code>, you should see:</p> <pre><code>{\n  \"status\": \"success\",\n  \"score\": 1.0,\n  \"evaluators_results\": [\n    {\n      \"evaluator_name\": \"AgentResponseEvaluator\",\n      \"status\": \"success\",\n      \"score\": 1.0,\n      ...\n    },\n    {\n      \"evaluator_name\": \"NetworkEventEvaluator\",\n      \"status\": \"success\",\n      \"score\": 1.0,\n      ...\n    }\n  ],\n  ...\n}\n</code></pre> <p>If you used the demo GitLab instance, you can now stop it:</p> <pre><code>uv run invoke -r examples demo-gitlab-stop\n</code></pre>"},{"location":"#where-to-next","title":"Where to Next?","text":"<ul> <li>Usage Guide - Agent workflow, batch evaluation, CLI filters, programmatic APIs</li> <li>Configuration Reference - All config options</li> <li>Evaluation Guide - Deep dive into evaluators and scoring</li> <li>API Reference - Type models and classes</li> </ul> <p>Happy benchmarking!</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This section documents the public interfaces exposed by WebArena-Verified.</p>"},{"location":"api_reference/#webarenaverified-api","title":"WebArenaVerified API","text":"<p>Access all framework functionality through the <code>WebArenaVerified</code> facade class. This provides a stable interface for task retrieval and evaluation.</p> <p>See the WebArenaVerified API reference for complete method documentation, or check the Usage Guide for practical examples.</p>"},{"location":"api_reference/#data-types","title":"Data Types","text":"<p>WebArena-Verified uses type-aware normalization for deterministic evaluation. The framework includes specialized types for dates, currency, URLs, coordinates, and more - each handling parsing and comparison without LLM-based evaluation.</p> <p>See the Data Types reference for the complete list of supported types and their normalization behavior.</p>"},{"location":"api_reference/webarena_verified/","title":"WebArenaVerified","text":"<p>Facade for WebArena Verified evaluation framework.</p> <p>This class provides a stable, high-level API. It is the recommended interface for all WebArena Verified operations, as it maintains API stability across versions.</p> Example <pre><code>from webarena_verified.api import WebArenaVerified\nfrom webarena_verified.types.config import WebArenaVerifiedConfig\n\n# Initialize with custom config\nconfig = WebArenaVerifiedConfig(\n    environments={\n        \"__GITLAB__\": {\n            \"urls\": [\"http://localhost:8012\"],\n            \"credentials\": {\"username\": \"root\", \"password\": \"demopass\"}\n        }\n    }\n)\nwa = WebArenaVerified(config=config)\n\n# Evaluate a task\nresult = wa.evaluate_task(\n    task_id=44,\n    agent_response=Path(\"output/44/agent_response.json\"),\n    network_trace=Path(\"output/44/network.har\")\n)\n</code></pre>"},{"location":"api_reference/webarena_verified/#webarena_verified.api.webarena_verified.WebArenaVerified.config","title":"config  <code>property</code>","text":"<pre><code>config: WebArenaVerifiedConfig\n</code></pre> <p>Access the configuration.</p>"},{"location":"api_reference/webarena_verified/#webarena_verified.api.webarena_verified.WebArenaVerified.evaluate_task","title":"evaluate_task","text":"<pre><code>evaluate_task(\n    *,\n    task_id: int,\n    agent_response: Any,\n    network_trace: list[dict] | Path | NetworkTrace,\n) -&gt; TaskEvalResult\n</code></pre> <p>Evaluate a single task with automatic format detection.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> <p>ID of the task to evaluate</p> required <code>agent_response</code> <code>Any</code> <p>Agent's response in any of these formats: - str: Raw response text (e.g., \"answer: 42\" or \"navigate: https://example.com\") - dict: Parsed response dict (e.g., {\"action\": \"retrieve\", \"value\": \"42\"}) - list: List of values (may result in validation failure) - None: No response (may result in validation failure) - Path: File path to read response from</p> required <code>network_trace</code> <code>list[dict] | Path | NetworkTrace</code> <p>Network trace in any of these formats: - Path: HAR file path - list: Pre-parsed list of network events/requests - NetworkTrace: Pre-constructed NetworkTrace object</p> required <p>Returns:</p> Type Description <code>TaskEvalResult</code> <p>TaskEvalResult with status, score, and detailed evaluation results.</p> <code>TaskEvalResult</code> <p>Errors are captured in result.status = EvalStatus.ERROR with result.error_msg.</p> <p>Examples:</p> <p>String response with HAR file: <pre><code>wa = WebArenaVerified()\nresult = wa.evaluate_task(\n    task_id=1,\n    agent_response=\"answer: 42\",\n    network_trace=Path(\"trace.har\")\n)\n</code></pre></p> <p>Dict response with pre-parsed trace: <pre><code>result = wa.evaluate_task(\n    task_id=1,\n    agent_response={\"action\": \"retrieve\", \"value\": \"42\"},\n    network_trace=network_events\n)\n</code></pre></p> <p>Response from file: <pre><code>result = wa.evaluate_task(\n    task_id=1,\n    agent_response=Path(\"response.txt\"),\n    network_trace=Path(\"trace.har\")\n)\n</code></pre></p>"},{"location":"api_reference/webarena_verified/#webarena_verified.api.webarena_verified.WebArenaVerified.get_task","title":"get_task","text":"<pre><code>get_task(task_id: int) -&gt; WebArenaVerifiedTask\n</code></pre> <p>Get a single task by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>task_id</code> <code>int</code> <p>Task ID to retrieve</p> required <p>Returns:</p> Type Description <code>WebArenaVerifiedTask</code> <p>WebArenaVerifiedTask instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If task not found</p> Example <pre><code>wa = WebArenaVerified()\ntask = wa.get_task(42)\nprint(task.intent)\n</code></pre>"},{"location":"api_reference/webarena_verified/#webarena_verified.api.webarena_verified.WebArenaVerified.get_tasks","title":"get_tasks","text":"<pre><code>get_tasks(\n    sites: list[WebArenaSite] | None = None,\n    template_id: int | None = None,\n    action: MainObjectiveType | None = None,\n) -&gt; list[WebArenaVerifiedTask]\n</code></pre> <p>Get all tasks, optionally filtered by criteria.</p> <p>Parameters:</p> Name Type Description Default <code>sites</code> <code>list[WebArenaSite] | None</code> <p>Filter by sites (default: None = no filter)</p> <code>None</code> <code>template_id</code> <code>int | None</code> <p>Filter by template ID (default: None = no filter)</p> <code>None</code> <code>action</code> <code>MainObjectiveType | None</code> <p>Filter by action type (default: None = no filter)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[WebArenaVerifiedTask]</code> <p>List of tasks matching all filter criteria (AND logic).</p> <code>list[WebArenaVerifiedTask]</code> <p>If all parameters are None, returns all tasks.</p> <p>Examples:</p> <p>Get all tasks: <pre><code>wa = WebArenaVerified()\nall_tasks = wa.get_tasks()\nprint(f\"Total tasks: {len(all_tasks)}\")\n</code></pre></p> <p>Filter by site: <pre><code>shopping_tasks = wa.get_tasks(sites=[WebArenaSite.SHOPPING])\n</code></pre></p> <p>Filter by multiple criteria: <pre><code>mutate_shopping = wa.get_tasks(\n    sites=[WebArenaSite.SHOPPING],\n    action=MainObjectiveType.MUTATE\n)\n</code></pre></p>"},{"location":"api_reference/data_types/","title":"Evaluation Data Types","text":"<p>The evaluation framework normalizes structured values before comparing them. Normalization is built on a registry of specialized data types that understand how to parse, coerce, and compare values such as dates, currencies, or geographic coordinates.</p>"},{"location":"api_reference/data_types/#registry-overview","title":"Registry Overview","text":"<p>The type registry lives in <code>src/webarena_verified/core/evaluation/data_types/__init__.py</code> and maps string identifiers (for example, <code>\"date\"</code> or <code>\"currency\"</code>) to concrete <code>NormalizedType</code> implementations.</p> Identifier Description Normalization Highlights <code>currency</code> Monetary amounts Handles currency symbols, separators, and precision. <code>date</code> Calendar dates Accepts multiple string formats and normalizes to ISO dates. <code>duration</code> Time durations Parses human-readable durations (e.g., <code>\\\"1h 30m\\\"</code>). <code>distance</code> Distance measurements Converts common distance units to a canonical value. <code>coordinates</code> Latitude and longitude pairs Applies tolerance-based comparison for geo points. <code>full_address</code> Street addresses Normalizes casing, whitespace, and common abbreviations. <code>url</code> Web URLs Standardizes schemes, hosts, and query parameters. <code>boolean</code> Boolean values Interprets <code>\\\"yes\\\"</code>, <code>\\\"true\\\"</code>, <code>\\\"1\\\"</code>, and related forms. <code>string</code> Plain strings Trims whitespace and normalizes casing when needed. <code>number</code> Numeric values Converts textual numbers to decimals for comparison. <code>null</code> Empty values Represents missing data explicitly. <p>Each specialized type inherits from <code>NormalizedType</code> and defines matching logic that is used by the <code>value_normalizer</code> and schema-based comparators (for example in the <code>NetworkEventEvaluator</code>).</p>"},{"location":"api_reference/data_types/agent_response/","title":"Agent Response Schema","text":"<p>This page documents the expected structure for agent responses in WebArena-Verified.</p>"},{"location":"api_reference/data_types/agent_response/#summary","title":"Summary","text":"<p>Agents must return a JSON object with the following structure:</p> Field Type Required Description <code>action</code> string Yes The action type performed: <code>retrieve</code>, <code>navigate</code>, or <code>mutate</code> <code>status</code> string Yes Outcome status: <code>SUCCESS</code> or error codes (<code>NOT_FOUND_ERROR</code>, <code>PERMISSION_DENIED_ERROR</code>, etc.) <code>results</code> array or null Yes Array of results for successful <code>retrieve</code> actions; <code>null</code> or empty array for navigate/mutate <code>error_details</code> string Optional Detailed explanation when status indicates failure; <code>null</code> for <code>SUCCESS</code>. Used for analysis and debugging only."},{"location":"api_reference/data_types/agent_response/#action-types","title":"Action Types","text":"Action Description When to Use <code>retrieve</code> Retrieved or accessed information When the agent reads data without making changes <code>mutate</code> Modified, created, or deleted data When the agent changes data in the environment <code>navigate</code> Navigated to a specific page or location When the agent moves to a target page or location"},{"location":"api_reference/data_types/agent_response/#status-codes","title":"Status Codes","text":"Status Description When to Use <code>SUCCESS</code> Task completed successfully When the action was completed as intended <code>NOT_FOUND_ERROR</code> Target entity doesn't exist When search criteria matched no results (e.g., issue, user, product not found) <code>ACTION_NOT_ALLOWED_ERROR</code> Platform doesn't support the action When the requested action is not supported <code>PERMISSION_DENIED_ERROR</code> Lack authorization When the agent doesn't have permission to perform the action <code>DATA_VALIDATION_ERROR</code> Input doesn't meet requirements When input is missing or has invalid format <code>UNKNOWN_ERROR</code> Unexpected failure When an unexpected error occurs that doesn't fit other categories. This is useful to catch cases where the testing environment is faulty (e.g., website is not reachable). This helps differentiate evaluation failure from evaluation framework errors."},{"location":"api_reference/data_types/agent_response/#results-field","title":"Results Field","text":"<p>The <code>results</code> field must follow these rules:</p> <ul> <li>For successful <code>retrieve</code> actions: Array containing the requested data</li> <li>For <code>navigate</code> or <code>mutate</code> actions: Must be <code>null</code> or empty array</li> <li>Array items: All items must be of the same type (string, number, boolean, object, or null)</li> <li>Object results: When returning multiple objects, all objects must have the same keys</li> </ul>"},{"location":"api_reference/data_types/agent_response/#example-responses","title":"Example Responses","text":"Successful RetrieveRetrieve with Structured DataSuccessful NavigateError Response <pre><code>{\n  \"action\": \"retrieve\",\n  \"status\": \"SUCCESS\",\n  \"results\": [\"Quest Lumaflex\u2122 Band\"]\n}\n</code></pre> <pre><code>{\n  \"action\": \"retrieve\",\n  \"status\": \"SUCCESS\",\n  \"results\": [\n    {\n      \"name\": \"Buffalo-Niagara International Airport\",\n      \"state\": \"New York\",\n      \"zip_code\": \"14225\"\n    }\n  ]\n}\n</code></pre> <pre><code>{\n  \"action\": \"navigate\",\n  \"status\": \"SUCCESS\",\n  \"results\": null\n}\n</code></pre> <pre><code>{\n  \"action\": \"retrieve\",\n  \"status\": \"NOT_FOUND_ERROR\",\n  \"results\": null,\n  \"error_details\": \"No products found matching the search criteria 'invalid-product-name' after checking all 5 pages of results.\"\n}\n</code></pre>"},{"location":"api_reference/data_types/agent_response/#python-model-reference","title":"Python Model Reference","text":"<p>The agent response schema is defined by the <code>FinalAgentResponse</code> class in the codebase:</p>"},{"location":"api_reference/data_types/agent_response/#webarena_verified.types.agent_response.FinalAgentResponse","title":"<code>FinalAgentResponse</code>","text":"<p>Final response format for agent task execution.</p> <p>The agent must respond with valid JSON containing the task type, task outcome status, retrieved data (for retrieve operations), and error details (when applicable).</p> <p>Attributes:</p> Name Type Description <code>task_type</code> <code>required</code> <p>The type of task performed (RETRIEVE, MUTATE, or NAVIGATE)</p> <code>status</code> <code>required</code> <p>The outcome of the task execution</p> <code>retrieved_data</code> <code>list[PublicResultItem] | None</code> <p>Array of items for 'retrieve' operations, null for 'mutate' and 'navigate' operations. Returns empty array if no items found. All items must be the same type (either all primitives of the same type, or all objects with the same keys). Use appropriate data type formats (e.g., numbers for amounts/counts, true/false for booleans, not strings). For list of objects, the user instruction contains the format specification.</p> <code>error_details</code> <code>str | None</code> <p>Null when status is 'SUCCESS'. Otherwise, explains used to explain the failure reason concisely.</p>"},{"location":"api_reference/data_types/agent_response_evaluator_cfg/","title":"AgentResponseEvaluatorCfg","text":"<p>Validates the agent's response structure, performed operation type, status, and retrieved data.</p> <p>Checks that the agent returns properly formatted responses with correct operation types (retrieve, navigate, mutate), status codes, and expected data values.</p> Example <pre><code>{\n    \"evaluator\": \"AgentResponseEvaluator\",\n    \"ordered\": false,\n    \"results_schema\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    \"expected\": {\n        \"task_type\": \"retrieve\",\n        \"status\": \"SUCCESS\",\n        \"retrieved_data\": [\"Product Name\"]\n    }\n}\n</code></pre> <p>Attributes:</p> Name Type Description <code>ordered</code> <code>bool</code> <p>Whether the retrieved data must match the expected order.</p> <code>results_schema</code> <code>SerializableMappingProxyType</code> <p>JSON schema defining the structure of the retrieved data array.</p> Source code in <code>src/webarena_verified/types/task.py</code> <pre><code>class AgentResponseEvaluatorCfg(BaseEval[_FinalAgentResponse]):\n    \"\"\"Validates the agent's response structure, performed operation type, status, and retrieved data.\n\n    Checks that the agent returns properly formatted responses with correct operation types\n    (retrieve, navigate, mutate), status codes, and expected data values.\n\n    Example:\n        ```json\n        {\n            \"evaluator\": \"AgentResponseEvaluator\",\n            \"ordered\": false,\n            \"results_schema\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n            \"expected\": {\n                \"task_type\": \"retrieve\",\n                \"status\": \"SUCCESS\",\n                \"retrieved_data\": [\"Product Name\"]\n            }\n        }\n        ```\n    \"\"\"\n\n    evaluator: Literal[\"AgentResponseEvaluator\"] = \"AgentResponseEvaluator\"\n\n    ordered: bool = False\n    \"\"\"Whether the retrieved data must match the expected order.\"\"\"\n\n    results_schema: SerializableMappingProxyType\n    \"\"\"JSON schema defining the structure of the retrieved data array.\"\"\"\n</code></pre>"},{"location":"api_reference/data_types/agent_response_evaluator_cfg/#src.webarena_verified.types.task.AgentResponseEvaluatorCfg.ordered","title":"<code>ordered = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the retrieved data must match the expected order.</p>"},{"location":"api_reference/data_types/agent_response_evaluator_cfg/#src.webarena_verified.types.task.AgentResponseEvaluatorCfg.results_schema","title":"<code>results_schema</code>  <code>instance-attribute</code>","text":"<p>JSON schema defining the structure of the retrieved data array.</p>"},{"location":"api_reference/data_types/config/","title":"Configuration","text":""},{"location":"api_reference/data_types/config/#webarenaverifiedconfig","title":"WebArenaVerifiedConfig","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.WebArenaVerifiedConfig","title":"<code>WebArenaVerifiedConfig</code>","text":"<p>Main configuration for WebArena-Verified evaluation framework.</p> <p>Specifies dataset location, environment configurations for each site, and file names for task outputs.</p> <p>Attributes:</p> Name Type Description <code>test_data_file</code> <code>Path</code> <p>Path to the WebArena-Verified dataset JSON file. Can be absolute or relative to project root (directory containing <code>.webarena_verified_root</code> or <code>pyproject.toml</code>). Defaults to <code>assets/dataset/webarena-verified.json</code>.</p> <code>environments</code> <code>dict[WebArenaSite, EnvironmentConfig] | None</code> <p>Maps site placeholder names (e.g., <code>__SHOPPING__</code>, <code>__GITLAB__</code>) to their environment configurations. Each environment contains URLs, credentials, and URL selection. Required for most operations. If None, evaluation operations requiring URL rendering will fail.</p> <code>agent_response_file_name</code> <code>str</code> <p>Filename for agent response JSON file containing the agent's final answer. Used directly without modification.</p> <code>trace_file_name</code> <code>str</code> <p>Filename for network trace file in HAR format, containing recorded browser network activity. Used directly without modification.</p> <code>eval_result_file_name</code> <code>str</code> <p>Filename for evaluation result JSON file containing pass/fail status and evaluation details. Used directly without modification.</p> <code>storage_state_file_name</code> <code>str</code> <p>Filename for browser storage state JSON file containing cookies and auth tokens for pre-login. Used directly without modification.</p> Example <pre><code>config = WebArenaVerifiedConfig.from_file(\"config.json\")\nenv = config.get_environment(WebArenaSite.SHOPPING)\nrendered = config.render_url(\"__SHOPPING__/products\", [WebArenaSite.SHOPPING])\n</code></pre> Source code in <code>src/webarena_verified/types/config.py</code> <pre><code>class WebArenaVerifiedConfig(BaseModel):\n    \"\"\"Main configuration for WebArena-Verified evaluation framework.\n\n    Specifies dataset location, environment configurations for each site, and file names\n    for task outputs.\n\n    Attributes:\n        test_data_file: Path to the WebArena-Verified dataset JSON file. Can be absolute or relative\n            to project root (directory containing `.webarena_verified_root` or `pyproject.toml`).\n            Defaults to `assets/dataset/webarena-verified.json`.\n\n        environments: Maps site placeholder names (e.g., `__SHOPPING__`, `__GITLAB__`) to their\n            environment configurations. Each environment contains URLs, credentials, and URL selection.\n            Required for most operations. If None, evaluation operations requiring URL rendering will fail.\n\n        agent_response_file_name: Filename for agent response JSON file containing the agent's\n            final answer. Used directly without modification.\n\n        trace_file_name: Filename for network trace file in HAR format, containing recorded browser\n            network activity. Used directly without modification.\n\n        eval_result_file_name: Filename for evaluation result JSON file containing pass/fail status\n            and evaluation details. Used directly without modification.\n\n        storage_state_file_name: Filename for browser storage state JSON file containing cookies\n            and auth tokens for pre-login. Used directly without modification.\n\n    Example:\n        ```python\n        config = WebArenaVerifiedConfig.from_file(\"config.json\")\n        env = config.get_environment(WebArenaSite.SHOPPING)\n        rendered = config.render_url(\"__SHOPPING__/products\", [WebArenaSite.SHOPPING])\n        ```\n    \"\"\"\n\n    test_data_file: Path = get_package_assets_path() / \"dataset/webarena-verified.json\"\n    environments: dict[WebArenaSite, EnvironmentConfig] | None = None\n\n    agent_response_file_name: str = \"agent_response.json\"\n\n    trace_file_name: str = \"network.har\"\n\n    eval_result_file_name: str = \"eval_result.json\"\n\n    storage_state_file_name: str = \".storage_state.json\"\n\n    @model_validator(mode=\"after\")\n    def validate_data(self) -&gt; Self:\n        \"\"\"Validate config and set default data file if needed.\"\"\"\n        if not self.test_data_file.exists():\n            # Handle cases where the run was done on a different machine or mount point\n            logger.warning(f\"test_data_file {self.test_data_file} does not exist. Falling back to default dataset.\")\n            self.test_data_file = get_package_assets_path() / \"dataset/webarena-verified.json\"\n        else:\n            logger.info(f\"Using test_data_file: {self.test_data_file}\")\n\n        return self\n\n    @classmethod\n    def from_file(cls, config_file: Path | str, test_data_file_override: Path | str | None = None) -&gt; Self:\n        \"\"\"Load config from file with optional test_data_file override.\n\n        Args:\n            config_file: Path to config JSON file\n            test_data_file_override: Optional path to override test_data_file after loading\n\n        Returns:\n            Loaded and validated config instance\n        \"\"\"\n        config_path = Path(config_file).resolve()\n        if not config_path.exists():\n            raise FileNotFoundError(f\"Config file {str(config_path)!r} does not exist.\")\n        try:\n            logger.info(f\"Loading config from: {str(config_path)!r}\")\n            raw = json.loads(config_path.read_text())\n            config = cls.model_validate(raw)\n\n            # Apply test_data_file override if provided\n            if test_data_file_override is not None:\n                config.test_data_file = Path(test_data_file_override)\n                logger.info(f\"Overriding test_data_file with: {test_data_file_override}\")\n\n            return config\n        except Exception:\n            logger.error(f\"Failed to load config. Check your configuration {str(config_path)!r}.\")\n            raise\n\n    def to_file(self, config_file: Path | str) -&gt; None:\n        \"\"\"Save configuration to JSON file.\n\n        Args:\n            config_file: Path to save the config file. Can be absolute or relative path.\n\n        Raises:\n            OSError: If file cannot be written (e.g., permission issues, invalid path)\n        \"\"\"\n        config_path = Path(config_file).resolve()\n        config_path.parent.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"Saving config to: {str(config_path)!r}\")\n        with open(config_path, \"w\") as f:\n            json.dump(self.model_dump(mode=\"json\"), f, indent=2)\n        logger.info(f\"Config saved successfully to: {str(config_path)!r}\")\n\n    def get_environment(self, site: WebArenaSite) -&gt; EnvironmentConfig | None:\n        \"\"\"Get environment configuration for a specific site.\"\"\"\n        if self.environments is None:\n            return None\n        return self.environments.get(site)\n\n    def render_url(\n        self,\n        url_w_template: str | list[str],\n        sites: list[WebArenaSite] | tuple[WebArenaSite, ...],\n        url_idx: int | None = None,\n        strict: bool = True,\n    ) -&gt; str | list[str]:\n        \"\"\"Render a URL by replacing the site template with the actual URL from the map.\n\n        Args:\n            url_w_template: URL template string or list of strings\n            sites: List or tuple of sites to try for rendering (in order)\n            url_idx: Optional URL index to use. If None, uses active_url_idx from environment config.\n            strict: If True, raise ValueError when no site matches. If False, return original template.\n\n        Returns:\n            Rendered URL with template replaced by actual URL. Returns same type as input (string or list).\n\n        Raises:\n            ValueError: If no environments configured, site not found in environments,\n                or (when strict=True) no site matches\n        \"\"\"\n        if self.environments is None:\n            raise ValueError(\"No environments configured\")\n\n        # Validate all sites exist in environments\n        if any(site not in self.environments for site in sites):\n            missing_sites = [site.name for site in sites if site not in self.environments]\n            raise ValueError(f\"Sites {missing_sites} not found in environments\")\n\n        # Convert input to list for uniform processing\n        was_single = not isinstance(url_w_template, list)\n        urls_to_process = [url_w_template] if was_single else url_w_template\n\n        # Process all URLs\n        results: list[str] = []\n        for url_template in urls_to_process:\n            assert isinstance(url_template, str)\n            rendered = None\n\n            # Try each site in order until one matches\n            for site in sites:\n                env_config = self.environments[site]\n                rendered_url = env_config.render_url(url_template, site, url_idx)\n                # Check if rendering actually happened (different from original)\n                if rendered_url != url_template:\n                    rendered = rendered_url\n                    break\n\n            if rendered is None:\n                if strict:\n                    raise ValueError(f\"No site in {[s.name for s in sites]} matched template: {url_template}\")\n                rendered = url_template  # Return original\n\n            results.append(rendered)\n\n        # Return single string if input was single, otherwise return list\n        return results[0] if was_single else results\n\n    def derender_url(\n        self,\n        url: str | list[str],\n        sites: list[WebArenaSite] | tuple[WebArenaSite, ...],\n        strict: bool = True,\n        is_ssh: bool = False,\n    ) -&gt; str | list[str]:\n        \"\"\"Derender a URL by replacing the actual URL with the site template from the map.\n\n        Args:\n            url: URL string or list of URL strings to derender\n            sites: List or tuple of sites to try for derendering (tried in order of URL specificity - longest first)\n            strict: If True, raise ValueError when no site matches. If False, return original URL.\n\n        Returns:\n            Derendered URL(s) with actual URL replaced by site template. Returns same type as input (string or list).\n\n        Raises:\n            ValueError: If no environments configured, site not found, or (when strict=True) URL doesn't match any site\n        \"\"\"\n        if self.environments is None:\n            raise ValueError(\"No environments configured\")\n\n        # Use local variable to narrow type for type checker\n        environments = self.environments\n\n        # Validate all sites exist in environments\n        if any(site not in environments for site in sites):\n            missing_sites = [site.name for site in sites if site not in environments]\n            raise ValueError(f\"Sites {missing_sites} not found in environments\")\n\n        # Sort sites by URL specificity (longest URL first) for accurate matching\n        sites_by_specificity = sorted(sites, key=lambda s: max(len(url) for url in environments[s].urls), reverse=True)\n\n        # Convert input to list for uniform processing\n        was_single = not isinstance(url, list)\n        urls_to_process = [url] if was_single else url\n\n        # Process all URLs\n        results: list[str] = []\n        for url_to_derender in urls_to_process:\n            assert isinstance(url_to_derender, str)\n            derendered = None\n\n            # Try each site (in specificity order) until one matches\n            for site in sites_by_specificity:\n                env_config = environments[site]\n                derendered_url = env_config.derender_url(url_to_derender, site, is_ssh=is_ssh)\n                if derendered_url is not None:\n                    derendered = derendered_url\n                    break\n\n            if derendered is None:\n                if strict:\n                    sites_str = [s.name for s in sites]\n                    raise ValueError(\n                        f\"URL '{url_to_derender}' does not match any configured URLs for sites {sites_str}\"\n                    )\n                derendered = url_to_derender  # Return original\n\n            results.append(derendered)\n\n        # Return single string if input was single, otherwise return list\n        return results[0] if was_single else results\n\n    def get_task_output_dir_metadata(self, output_dir: Path, task_id: int) -&gt; TaskOutputDirMeta:\n        \"\"\"Create TaskOutputDirMeta for a task using this config's file names.\n\n        Creates the task output directory and computes all file paths from file names.\n\n        Args:\n            output_dir: Base output directory\n            task_id: Task identifier\n\n        Returns:\n            TaskOutputDirMeta instance with all paths pre-computed\n\n        Note:\n            This method is maintained for backward compatibility. New code should use\n            TaskOutputDirMeta.create(config, output_dir, task_id) directly.\n        \"\"\"\n        return TaskOutputDirMeta.create(self, output_dir, task_id)\n</code></pre>"},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.WebArenaVerifiedConfig.test_data_file","title":"<code>test_data_file = get_package_assets_path() / 'dataset/webarena-verified.json'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.WebArenaVerifiedConfig.environments","title":"<code>environments = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.WebArenaVerifiedConfig.agent_response_file_name","title":"<code>agent_response_file_name = 'agent_response.json'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.WebArenaVerifiedConfig.trace_file_name","title":"<code>trace_file_name = 'network.har'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.WebArenaVerifiedConfig.eval_result_file_name","title":"<code>eval_result_file_name = 'eval_result.json'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.WebArenaVerifiedConfig.storage_state_file_name","title":"<code>storage_state_file_name = '.storage_state.json'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#environmentconfig","title":"EnvironmentConfig","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.EnvironmentConfig","title":"<code>EnvironmentConfig</code>","text":"<p>Configuration for a single environment/site.</p> <p>Supports multiple environment variations (e.g., staging, production) for the same site. When evaluating across multiple test environments, any URL in the list can be mapped back to the template placeholder, allowing you to evaluate logs from different environments using the same task definitions.</p> <p>Attributes:</p> Name Type Description <code>urls</code> <code>list[str]</code> <p>List of all possible URLs for this environment (e.g., production, staging, dev instances). During evaluation, any URL in this list can be derendered to the same template placeholder. Example: Both <code>https://gitlab-staging.com</code> and <code>https://gitlab-prod.com</code> can map to <code>__GITLAB__</code>.</p> <code>active_url_idx</code> <code>int | None</code> <p>Selects which URL from the <code>urls</code> array to use when running tasks (default: 0). This determines which environment the task will execute against. When transforming template URLs (e.g., <code>__GITLAB__</code>) back to real URLs (e.g., <code>https://gitlab.example.com</code>), the URL at this index is used. Defaults to 0 (first URL) if not specified.</p> <code>use_header_login</code> <code>bool</code> <p>Optional flag to enable header-based authentication for this environment. When True, authentication is performed via HTTP headers (e.g., X-M2-Admin-Auto-Login-User) instead of traditional UI login. Only supported for certain sites (e.g., shopping_admin). Defaults to False.</p> <code>credentials</code> <code>dict[str, str] | None</code> <p>Optional authentication credentials for this environment (e.g., username, password). Used for automated login before task execution.</p> <code>extra</code> <code>dict[str, Any]</code> <p>Additional configuration data for this environment as a flexible key-value store. Can hold any site-specific configuration (e.g., {\"container_name\": \"my-container\"}).</p> Example <p>If you have logs from staging (<code>https://gitlab-staging.com</code>) and production (<code>https://gitlab-prod.com</code>), both can be derendered to <code>__GITLAB__</code> during evaluation. The <code>active_url_idx</code> determines which variant is used when rendering templates to real URLs.</p> Source code in <code>src/webarena_verified/types/config.py</code> <pre><code>class EnvironmentConfig(BaseModel):\n    \"\"\"Configuration for a single environment/site.\n\n    Supports multiple environment variations (e.g., staging, production) for the same site.\n    When evaluating across multiple test environments, any URL in the list can be mapped back\n    to the template placeholder, allowing you to evaluate logs from different environments\n    using the same task definitions.\n\n    Attributes:\n        urls: List of all possible URLs for this environment (e.g., production, staging, dev instances).\n            During evaluation, any URL in this list can be derendered to the same template placeholder.\n            Example: Both `https://gitlab-staging.com` and `https://gitlab-prod.com` can map to `__GITLAB__`.\n\n        active_url_idx: Selects which URL from the `urls` array to use when running tasks (default: 0).\n            This determines which environment the task will execute against. When transforming template\n            URLs (e.g., `__GITLAB__`) back to real URLs (e.g., `https://gitlab.example.com`), the URL at\n            this index is used. Defaults to 0 (first URL) if not specified.\n\n        use_header_login: Optional flag to enable header-based authentication for this environment.\n            When True, authentication is performed via HTTP headers (e.g., X-M2-Admin-Auto-Login-User) instead of\n            traditional UI login. Only supported for certain sites (e.g., shopping_admin). Defaults to False.\n\n        credentials: Optional authentication credentials for this environment (e.g., username, password).\n            Used for automated login before task execution.\n\n        extra: Additional configuration data for this environment as a flexible key-value store.\n            Can hold any site-specific configuration (e.g., {\"container_name\": \"my-container\"}).\n\n    Example:\n        If you have logs from staging (`https://gitlab-staging.com`) and production\n        (`https://gitlab-prod.com`), both can be derendered to `__GITLAB__` during evaluation.\n        The `active_url_idx` determines which variant is used when rendering templates to real URLs.\n    \"\"\"\n\n    urls: list[str]\n\n    active_url_idx: int | None = None\n\n    use_header_login: bool = False\n\n    credentials: dict[str, str] | None = None\n\n    extra: dict[str, Any] = Field(default_factory=dict)\n\n    @model_validator(mode=\"after\")\n    def set_default_active_url_idx(self) -&gt; Self:\n        \"\"\"Set active_url_idx to 0 if None and urls list is not empty.\"\"\"\n        if self.active_url_idx is None and self.urls:\n            self.active_url_idx = 0\n        return self\n\n    @property\n    def active_url(self) -&gt; str | None:\n        \"\"\"Get the active URL for this environment.\n\n        Returns:\n            The active URL string, or None if urls list is empty.\n        \"\"\"\n        if not self.urls or self.active_url_idx is None:\n            return None\n        return self.urls[self.active_url_idx]\n\n    def set_active_url(self, new_url: str) -&gt; None:\n        \"\"\"Set the active URL by adding it to urls if not present and updating the index.\n\n        If the URL already exists in the list, it will be set as active without duplication.\n\n        Args:\n            new_url: The URL to set as active\n        \"\"\"\n        if new_url in self.urls:\n            self.active_url_idx = self.urls.index(new_url)\n        else:\n            self.urls.append(new_url)\n            self.active_url_idx = len(self.urls) - 1\n\n    def render_url(self, url_template: str, site: WebArenaSite, url_idx: int | None = None) -&gt; str:\n        \"\"\"Render a URL template by replacing the site template with this environment's URL.\n\n        Args:\n            url_template: URL template string (e.g., \"__GITLAB__/api/v1\")\n            site: The site enum for this environment\n            url_idx: Optional URL index to use. If None, uses active_url_idx.\n\n        Returns:\n            Rendered URL with template replaced by actual URL from this environment.\n            Returns original template unchanged if it doesn't match this site's template.\n        \"\"\"\n        if not url_template.startswith(site.url_name_template):\n            return url_template\n\n        idx = url_idx if url_idx is not None else self.active_url_idx\n        if idx is None or idx &lt; 0 or idx &gt;= len(self.urls):\n            raise ValueError(\n                f\"Invalid environment URL index {idx} for site {site.name}. Verify that config file has url for {site}.\"\n            )\n\n        url = self.urls[idx]  # currently active URL\n        return url_template.replace(site.url_name_template, url)\n\n    def derender_url(self, url: str, site: WebArenaSite, is_ssh: bool = False) -&gt; str | None:\n        \"\"\"Derender a URL by replacing the actual URL with the site template.\n\n        Args:\n            url: URL string to derender\n            site: The site enum for this environment\n            is_ssh: If True, treat as SSH URL and use SSH-specific derendering\n\n        Returns:\n            Derendered URL with actual URL replaced by site template, or None if no match found.\n        \"\"\"\n        for site_url in self.urls:\n            if is_ssh:\n                host_name = urlparse(site_url).hostname\n                if host_name:\n                    result = self._derender_ssh_url(url, host_name, site.url_name_template)\n                    if result is not None:\n                        return result\n            elif site_url in url:\n                return url.replace(site_url.rstrip(\"/\"), site.url_name_template)\n        return None\n\n    def _derender_ssh_url(self, ssh_url: str, hostname: str, template: str) -&gt; str | None:\n        \"\"\"Derender SSH URL by replacing hostname with __ssh_host__ and converting to short format.\n\n        Args:\n            ssh_url: SSH URL to derender (e.g., \"ssh://git@localhost:2222/path/to/repo.git\")\n            hostname: Hostname to match and replace (e.g., \"localhost\")\n            template: Present for interface consistency; ignored for SSH URLs,\n                which always use the fixed __ssh_host__ template\n\n        Returns:\n            Derendered URL in short format (e.g., \"git@__ssh_host__:path/to/repo.git\") or None if hostname not found\n        \"\"\"\n        # Pattern: capture ssh://user@, hostname, optional port, and path\n        # We need to match the specific hostname and replace it with template\n        pattern = rf\"(ssh://([^@]+)@){re.escape(hostname)}(:[0-9]+)?(/.*)?$\"\n        match = re.match(pattern, ssh_url)\n\n        if not match:\n            return None\n\n        # Extract components\n        user = match.group(2)  # git\n        path = match.group(4) or \"\"  # /path/to/repo.git\n\n        # Remove leading slashes from path for short format\n        path = path.lstrip(\"/\")\n\n        # Convert to short SCP-like format with generic __ssh_host__ template\n        # Use __ssh_host__ instead of site-specific template for SSH URLs\n        return f\"{user}@__ssh_host__:{path}\"\n</code></pre>"},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.EnvironmentConfig.urls","title":"<code>urls</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.EnvironmentConfig.active_url_idx","title":"<code>active_url_idx = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#src.webarena_verified.types.config.EnvironmentConfig.credentials","title":"<code>credentials = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/config/#supported-site-placeholders","title":"Supported Site Placeholders","text":"Placeholder Description <code>__SHOPPING_ADMIN__</code> Shopping admin dashboard <code>__SHOPPING__</code> Shopping customer site <code>__REDDIT__</code> Reddit-like forum <code>__GITLAB__</code> GitLab instance <code>__WIKIPEDIA__</code> Wikipedia instance <code>__MAP__</code> Map application <code>__HOMEPAGE__</code> Homepage/portal"},{"location":"api_reference/data_types/task/","title":"WebArenaVerifiedTask","text":""},{"location":"api_reference/data_types/task/#attributes","title":"Attributes","text":"<p>Attributes:</p> Name Type Description <code>sites</code> <code>tuple[WebArenaSite, ...]</code> <p>List of platforms involved (e.g., gitlab, shopping_admin).</p> <code>task_id</code> <code>int</code> <p>Unique identifier for the task.</p> <code>start_urls</code> <code>tuple[NonEmptyStr, ...]</code> <p>Initial URLs where the task begins.</p> <code>intent</code> <code>NonEmptyStr</code> <p>Natural language description of what to accomplish.</p> <code>intent_template</code> <code>NonEmptyStr</code> <p>Template with placeholders (e.g., 'Get top-{{n}} products').</p> <code>instantiation_dict</code> <code>SerializableMappingProxyType</code> <p>Values used to fill template placeholders.</p> <code>eval</code> <code>tuple[EvaluatorCfg, ...]</code> <p>Array of evaluator configurations.</p> <code>revision</code> <code>Annotated[int, Field(ge=1)]</code> <p>Integer revision number tracking task changes (minimum 1).</p> Source code in <code>src/webarena_verified/types/task.py</code> <pre><code>class WebArenaVerifiedTask(BaseModel):\n    \"\"\"Pydantic model for a WebArena Verified task.\"\"\"\n\n    sites: tuple[WebArenaSite, ...]\n    \"\"\"List of platforms involved (e.g., gitlab, shopping_admin).\"\"\"\n\n    task_id: int\n    \"\"\"Unique identifier for the task.\"\"\"\n\n    intent_template_id: int\n    \"\"\"Groups tasks from the same template.\"\"\"\n\n    start_urls: tuple[NonEmptyStr, ...]\n    \"\"\"Initial URLs where the task begins.\"\"\"\n\n    intent: NonEmptyStr\n    \"\"\"Natural language description of what to accomplish.\"\"\"\n\n    eval: tuple[EvaluatorCfg, ...]\n    \"\"\"Array of evaluator configurations.\"\"\"\n\n    intent_template: NonEmptyStr\n    \"\"\"Template with placeholders (e.g., 'Get top-{{n}} products').\"\"\"\n\n    instantiation_dict: SerializableMappingProxyType\n    \"\"\"Values used to fill template placeholders.\"\"\"\n\n    revision: Annotated[int, Field(ge=1)]\n    \"\"\"Integer revision number tracking task changes (minimum 1).\"\"\"\n\n    model_config = ConfigDict(\n        frozen=True,\n        extra=\"forbid\",\n        arbitrary_types_allowed=True,\n    )\n\n    @model_validator(mode=\"after\")\n    def check_eval_has_agent_response(self) -&gt; Self:\n        \"\"\"Validate that eval contains at least one AgentResponseEval item.\"\"\"\n        if not any(isinstance(item, AgentResponseEvaluatorCfg) for item in self.eval):\n            raise ValueError(\"eval must contain at least one AgentResponseEval item\")\n        return self\n\n    @property\n    def expected_agent_response(self) -&gt; FinalAgentResponse:\n        \"\"\"Return the expected agent response from the first AgentResponseEval.\"\"\"\n        for item in self.eval:\n            if isinstance(item, AgentResponseEvaluatorCfg):\n                return item.expected\n        raise ValueError(\"No AgentResponseEval found in eval\")\n\n    @property\n    def expected_action(self) -&gt; str:\n        \"\"\"Return the expected task type from the expected agent response.\"\"\"\n        return self.expected_agent_response.task_type\n\n    @property\n    def network_event_evaluator_cfgs(self) -&gt; tuple[NetworkEventEvaluatorCfg, ...]:\n        \"\"\"Return all NetworkEventEvaluatorCfg items in eval.\"\"\"\n        return tuple(item for item in self.eval if isinstance(item, NetworkEventEvaluatorCfg))\n\n    @property\n    def is_navigate_task(self) -&gt; bool:\n        \"\"\"Check if this is a navigate task.\"\"\"\n        return self.expected_agent_response.is_navigate\n\n    @property\n    def is_mutate_task(self) -&gt; bool:\n        \"\"\"Check if this is a mutate task.\"\"\"\n        return self.expected_agent_response.is_mutate\n\n    @property\n    def is_retrieve_task(self) -&gt; bool:\n        \"\"\"Check if this is a retrieve task.\"\"\"\n        return self.expected_agent_response.is_retrieve\n\n    @property\n    def sites_str(self) -&gt; str:\n        \"\"\"Return a comma-separated string of site names.\"\"\"\n        return \"-\".join(sorted([site.value for site in self.sites]))\n\n    def __str__(self) -&gt; str:\n        \"\"\"Pretty print task with key information.\"\"\"\n        return (\n            f\"WebArenaVerifiedTask(\\n\"\n            f\"  task_id={self.task_id},\\n\"\n            f\"  intent_template_id={self.intent_template_id},\\n\"\n            f\"  sites={list(self.sites)},\\n\"\n            f\"  intent={self.intent!r},\\n\"\n            f\"  start_urls={list(self.start_urls)},\\n\"\n            f\")\"\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Repr with key information.\"\"\"\n        return (\n            f\"WebArenaVerifiedTask(task_id={self.task_id}, \"\n            f\"intent_template_id={self.intent_template_id}, sites=[{self.sites_str}])\"\n        )\n</code></pre>"},{"location":"api_reference/data_types/task/#src.webarena_verified.types.task.WebArenaVerifiedTask.sites","title":"<code>sites</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task/#src.webarena_verified.types.task.WebArenaVerifiedTask.task_id","title":"<code>task_id</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task/#src.webarena_verified.types.task.WebArenaVerifiedTask.start_urls","title":"<code>start_urls</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task/#src.webarena_verified.types.task.WebArenaVerifiedTask.intent","title":"<code>intent</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task/#src.webarena_verified.types.task.WebArenaVerifiedTask.intent_template","title":"<code>intent_template</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task/#src.webarena_verified.types.task.WebArenaVerifiedTask.instantiation_dict","title":"<code>instantiation_dict</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task/#src.webarena_verified.types.task.WebArenaVerifiedTask.eval","title":"<code>eval</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task/#src.webarena_verified.types.task.WebArenaVerifiedTask.revision","title":"<code>revision</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task/#example-task","title":"Example Task","text":"<pre><code>{\n  \"task_id\": 7,\n  \"intent_template_id\": 79,\n  \"sites\": [\"map\"],\n  \"start_urls\": [\"__MAP__\"],\n  \"intent\": \"Get the name, state, and zip code of all international airports that are within a driving distance of 50 km to Carnegie Mellon University. Use \\\"name\\\" for the name, \\\"state\\\" for the state, and \\\"postcode\\\" for the postcode.\",\n  \"intent_template\": \"Get the name, state, and zip code of all {{airport_type}} that are within a driving distance of {{radius}} to {{start}}. {{retrieved_data_format_spec}}.\",\n  \"instantiation_dict\": {\n    \"airport_type\": \"international airports\",\n    \"start\": \"Carnegie Mellon University\",\n    \"radius\": \"50 km\",\n    \"retrieved_data_format_spec\": \"Use \\\"name\\\" for the name, \\\"state\\\" for the state, and \\\"postcode\\\" for the postcode\"\n  },\n  \"eval\": [\n    {\n      \"evaluator\": \"AgentResponseEvaluator\",\n      \"ordered\": false,\n      \"results_schema\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"state\": {\"type\": \"string\"},\n            \"postcode\": {\"type\": \"string\"}\n          }\n        }\n      },\n      \"expected\": {\n        \"task_type\": \"retrieve\",\n        \"status\": \"SUCCESS\",\n        \"retrieved_data\": [\n          {\n            \"name\": \"Pittsburgh International Airport\",\n            \"state\": \"Pennsylvania\",\n            \"postcode\": \"15231\"\n          }\n        ]\n      }\n    }\n  ],\n  \"revision\": 2\n}\n</code></pre>"},{"location":"api_reference/data_types/task_eval_context/","title":"TaskEvalContext","text":"<p>The evaluation context passed to evaluators containing all data needed to validate task completion.</p>"},{"location":"api_reference/data_types/task_eval_context/#overview","title":"Overview","text":"<p><code>TaskEvalContext</code> is provided to evaluators during task evaluation. It contains the task definition, agent response, network trace, and configuration needed for validation.</p>"},{"location":"api_reference/data_types/task_eval_context/#attributes","title":"Attributes","text":"<p>Attributes:</p> Name Type Description <code>task</code> <code>WebArenaVerifiedTask</code> <code>agent_response_raw</code> <code>Any | TransformedAgentResponse | None</code> <code>network_trace</code> <code>NetworkTrace</code> <code>config</code> <code>WebArenaVerifiedConfig</code> Source code in <code>src/webarena_verified/types/eval.py</code> <pre><code>class TaskEvalContext(BaseModel):\n    \"\"\"Context passed to evaluators during task evaluation.\"\"\"\n\n    task: WebArenaVerifiedTask\n    agent_response_raw: Any | TransformedAgentResponse | None = None\n    network_trace: NetworkTrace\n    config: WebArenaVerifiedConfig\n\n    model_config = ConfigDict(frozen=True, extra=\"forbid\", arbitrary_types_allowed=True)\n</code></pre>"},{"location":"api_reference/data_types/task_eval_context/#src.webarena_verified.types.eval.TaskEvalContext.task","title":"<code>task</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task_eval_context/#src.webarena_verified.types.eval.TaskEvalContext.agent_response_raw","title":"<code>agent_response_raw = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task_eval_context/#src.webarena_verified.types.eval.TaskEvalContext.network_trace","title":"<code>network_trace</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task_eval_context/#src.webarena_verified.types.eval.TaskEvalContext.config","title":"<code>config</code>  <code>instance-attribute</code>","text":""},{"location":"api_reference/data_types/task_eval_context/#field-descriptions","title":"Field Descriptions","text":"Field Type Description <code>task</code> <code>WebArenaVerifiedTask</code> The task being evaluated, including expected values and evaluator configurations <code>agent_response_raw</code> <code>Any</code> The raw agent response (string, dict, or parsed JSON). Used by AgentResponseEvaluator <code>network_trace</code> <code>NetworkTrace</code> Captured network events from the agent's execution. Used by NetworkEventEvaluator <code>config</code> <code>WebArenaVerifiedConfig</code> Framework configuration including site URLs and settings"},{"location":"api_reference/data_types/task_eval_context/#usage-in-evaluators","title":"Usage in Evaluators","text":"<p>Different evaluators access different fields from the context:</p> <ul> <li>AgentResponseEvaluator: Validates the structured response by accessing <code>agent_response_raw</code></li> <li>NetworkEventEvaluator: Validates network traffic by accessing <code>network_trace</code></li> </ul>"},{"location":"api_reference/data_types/task_eval_context/#see-also","title":"See Also","text":"<ul> <li>Evaluation Results - Understanding evaluator output format</li> <li>WebArenaVerifiedTask - Task structure and definition</li> </ul>"},{"location":"api_reference/data_types/web_arena_site/","title":"WebArenaSite","text":"<p>Enum defining the supported web platforms in the WebArena benchmark.</p>"},{"location":"api_reference/data_types/web_arena_site/#enum-values","title":"Enum Values","text":"Name Value <code>GITLAB</code> <code>\"gitlab\"</code> <code>MAP</code> <code>\"map\"</code> <code>REDDIT</code> <code>\"reddit\"</code> <code>SHOPPING_ADMIN</code> <code>\"shopping_admin\"</code> <code>SHOPPING</code> <code>\"shopping\"</code> <code>WIKIPEDIA</code> <code>\"wikipedia\"</code> <code>HOMEPAGE</code> <code>\"homepage\"</code>"},{"location":"api_reference/evaluators/agent_response_evaluator/","title":"AgentResponseEvaluator","text":"<p>Ensures the agent returns a structured response with the expected action, status, and payload.</p>"},{"location":"api_reference/evaluators/agent_response_evaluator/#configuration","title":"Configuration","text":"Field Type Notes <code>evaluator</code> <code>\"AgentResponseEvaluator\"</code> Required discriminator. <code>ordered</code> <code>bool</code> If <code>True</code>, <code>retrieved_data</code> order must match. <code>results_schema</code> JSON Schema Validates <code>retrieved_data</code> shape. <code>expected</code> <code>FinalAgentResponse</code> Expected action, status, results, and optional error details. <p>See <code>AgentResponseEvaluatorCfg</code> for the full model.</p>"},{"location":"api_reference/evaluators/agent_response_evaluator/#example","title":"Example","text":"<pre><code>{\n  \"evaluator\": \"AgentResponseEvaluator\",\n  \"ordered\": false,\n  \"results_schema\": {\n    \"type\": \"array\",\n    \"items\": {\"type\": \"string\"}\n  },\n  \"expected\": {\n    \"task_type\": \"retrieve\",\n    \"status\": \"SUCCESS\",\n    \"retrieved_data\": [\"Quest Lumaflex\u2122 Band\"]\n  }\n}\n</code></pre> <p>The evaluator compares the normalized agent response against <code>expected</code> and reports failures when:</p> <ul> <li>The action differs (<code>retrieve</code>, <code>navigate</code>, <code>mutate</code>).</li> <li>The status mismatches (<code>SUCCESS</code>, <code>NOT_FOUND_ERROR</code>, etc.).</li> <li>The payload violates the schema or expected values.</li> </ul>"},{"location":"api_reference/evaluators/network_event_evaluator/","title":"NetworkEventEvaluator","text":"<p>Validates captured network traffic to confirm the agent hit the right endpoints with the expected status, headers, and parameters.</p>"},{"location":"api_reference/evaluators/network_event_evaluator/#configuration","title":"Configuration","text":"Field Type Default Notes <code>evaluator</code> <code>\"NetworkEventEvaluator\"</code> \u2014 Required discriminator. <code>url_match_mode</code> <code>\"exact\" \\| \"prefix\" \\| \"regex\"</code> <code>\"exact\"</code> How URLs are matched. <code>last_event_only</code> <code>bool</code> <code>true</code> Validate only the most recent matching event. <code>ignored_query_params</code> <code>tuple[str, ...]</code> \u2014 Literal query keys to drop before comparison. <code>ignored_query_params_patterns</code> <code>tuple[str, ...]</code> \u2014 Regex patterns for keys to drop. <code>decode_base64_query</code> <code>bool</code> <code>false</code> Decode base64 segments embedded in URL paths. <code>query_params_schema</code> / <code>post_data_schema</code> JSON Schema \u2014 Optional type-aware normalization. <code>expected</code> <code>NetworkEventExpected</code> \u2014 Required block describing the target event. <p><code>NetworkEventExpected</code> supports:</p> Field Type Default <code>url</code> <code>str</code> \u2014 <code>headers</code> <code>dict[str, str]</code> <code>null</code> <code>query_params</code> <code>dict[str, list[str]]</code> <code>null</code> <code>post_data</code> <code>dict[str, str]</code> <code>null</code> <code>response_status</code> <code>int</code> <code>200</code> <code>event_type</code> <code>\"navigation\" \\| \"modification\" \\| \"other\" \\| null</code> <code>null</code> <code>http_method</code> <code>str</code> <code>\"GET\"</code>"},{"location":"api_reference/evaluators/network_event_evaluator/#example","title":"Example","text":"<pre><code>{\n  \"evaluator\": \"NetworkEventEvaluator\",\n  \"url_match_mode\": \"prefix\",\n  \"ignored_query_params_patterns\": [\"^paging\", \"^sorting\"],\n  \"expected\": {\n    \"url\": \"__SHOPPING_ADMIN__/mui/index/render/\",\n    \"headers\": {\n      \"referer\": \"__SHOPPING_ADMIN__/sales/order/\",\n      \"X-Requested-With\": \"XMLHttpRequest\"\n    },\n    \"query_string\": {\n      \"namespace\": \"sales_order_grid\"\n    },\n    \"response_status\": 200,\n    \"event_type\": \"navigation\",\n    \"http_method\": \"GET\"\n  }\n}\n</code></pre> <p>The evaluator searches captured events using the configured URL mode and headers. It then checks the last matching event (or any, if <code>last_event_only</code> is <code>false</code>) against the expected method, status, headers, query string, and optional schemas.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>See GitHub Releases for version history and release notes.</p>"},{"location":"changelog/v1.0.0-rc.1/","title":"v1.0.0","text":""},{"location":"changelog/v1.0.0-rc.1/#data-schema-changes","title":"Data Schema Changes","text":"<p>We use WebArenaVerifiedTask, a Pydantic <code>BaseModel</code>, to map and interact with task data. Pydantic provides automatic validation, type safety, and seamless JSON serialization/deserialization, ensuring data integrity throughout the benchmark pipeline.</p>"},{"location":"changelog/v1.0.0-rc.1/#changes-from-webarena","title":"Changes from WebArena","text":"<p>WebArena-Verified is a verified benchmark introducing a more structured and extensible task format compared to the original WebArena benchmark. This section outlines the key changes.</p>"},{"location":"changelog/v1.0.0-rc.1/#high-level-changes","title":"High-Level Changes","text":"Aspect WebArena WebArena-Verified Evaluation Structure Single dict with type strings Array of typed evaluator configs Field Organization Flat structure with runtime configs Separation of task data and runtime concerns Type Safety String-based types Discriminated unions with Pydantic models Extensibility Limited evaluator types Multiple evaluator types with clear interfaces"},{"location":"changelog/v1.0.0-rc.1/#field-by-field-mapping","title":"Field-by-Field Mapping","text":"WebArena Field WebArena-Verified Field Rationale <code>start_url</code> (string) <code>start_urls</code> (array) Support multiple starting URLs for complex tasks <code>eval</code> (object) <code>eval</code> (array of objects) Simplify and flatten evaluator structure for extensibility <code>eval.eval_types</code> <code>eval[].evaluator</code> More explicit evaluator identification <code>eval.reference_answers</code> <code>eval[].expected</code> Structured expected values per evaluator type <code>eval.reference_url</code> <code>NetworkEventEvaluator</code> Replaced string matching with network trace validation <code>eval.program_html</code> <code>NetworkEventEvaluator</code> Network traces now cover backend checks <code>require_login</code> (removed) Runtime configuration, not task data <code>storage_state</code> (removed) Runtime configuration (local file path) <code>geolocation</code> (removed) Always null in dataset; removed as redundant <code>require_reset</code> (removed) Always false in dataset; removed as redundant - <code>format_specification</code> New: Specifies output format requirements - <code>start_url_context</code> New: Provides context about starting page - <code>revision</code> New: Integer revision number for task changes (minimum 1)"},{"location":"changelog/v1.0.0-rc.1/#evaluation-system-migration","title":"Evaluation System Migration","text":"<p>The evaluation system was restructured to use typed evaluators instead of string-based types:</p>"},{"location":"changelog/v1.0.0-rc.1/#webarena-evaluation-types-webarena-verified-evaluators","title":"WebArena Evaluation Types \u2192 WebArena-Verified Evaluators","text":"WebArena <code>eval_types</code> WebArena-Verified Evaluator(s) Purpose <code>string_match</code> <code>AgentResponseEvaluator</code> (action=retrieve) Validates agent's returned response format, action type, and result values <code>program_html</code> (persisted changes) <code>NetworkEventEvaluator</code> Validates backend-side mutations via network traces <code>program_html</code> (not persisted changes) <code>NetworkEventEvaluator</code> Network requests capture transient UI interactions <code>url_match</code> <code>NetworkEventEvaluator</code> + <code>AgentResponseEvaluator</code> (action=navigate) Validates navigation to correct URL using network traces"},{"location":"changelog/v1.0.0-rc.1/#expected-agent-response-changes","title":"Expected Agent Response Changes","text":"<p>WebArena-Verified introduces a structured agent response format, replacing the plain string format used in WebArena.</p>"},{"location":"changelog/v1.0.0-rc.1/#webarena-agent-response","title":"WebArena Agent Response","text":"<p>Agents returned a plain string containing the answer:</p> <pre><code>Quest Lumaflex\u2122 Band\n</code></pre>"},{"location":"changelog/v1.0.0-rc.1/#webarena-verified-agent-response","title":"WebArena-Verified Agent Response","text":"<p>Agents return a structured JSON object with explicit action type, status, and results:</p> <pre><code>{\n  \"action\": \"retrieve\",\n  \"status\": \"SUCCESS\",\n  \"results\": [\"Quest Lumaflex\u2122 Band\"]\n}\n</code></pre>"},{"location":"changelog/v1.0.0-rc.1/#benefits","title":"Benefits","text":"<ul> <li>Reduced false negatives: Eliminates evaluation failures due to string parsing ambiguities.</li> <li>Explicit status indication: Agents clearly report whether they succeeded or encountered errors. This is especially useful when an agent reaches the maximum number of iterations for navigation tasks but fails at the right page. Without explicit status, the evaluation would incorrectly pass.</li> <li>Action type tracking: Clear indication of the performed action (<code>retrieve</code>, <code>navigate</code>, or <code>mutate</code>). This is especially beneficial to differentiate between navigate and retrieve tasks. For example, a task might expect the agent to navigate, but the agent thinks it needs to retrieve some value. The agent might fail at retrieving the value but still navigate to the right page. Without the action being explicit, the validation would incorrectly pass.</li> </ul>"},{"location":"changelog/v1.0.0-rc.1/#result-format-specification","title":"Result Format Specification","text":"<p>WebArena-Verified introduces the <code>format_specification</code> field to eliminate ambiguity in how agents should format their results.</p>"},{"location":"changelog/v1.0.0-rc.1/#problem-in-webarena","title":"Problem in WebArena","text":"<p>Without format specifications, agents had to interpret how to format results, leading to evaluation ambiguities. For example, Task ID 10 asks: \"Tell me the full address of all US international airports that are within a driving distance of 60 km to Niagara Falls\"</p> <p>Different agents interpreted \"full address\" differently, producing varied formats that were difficult to evaluate consistently. One leaderboard agent returned:</p> <pre><code>There is one US international airport within 60 km driving distance of Niagara Falls: Buffalo-Niagara International Airport, Holtz Drive, Town of Cheektowaga, Erie County, New York, 14225, United States.\n</code></pre>"},{"location":"changelog/v1.0.0-rc.1/#solution-in-webarena-verified","title":"Solution in WebArena-Verified","text":"<p>The <code>format_specification</code> field explicitly defines the expected result structure. For Task ID 10, the format specification states: \"Use \\\"name\\\" for the name, \\\"state\\\" for the state, and \\\"zip_code\\\" for the zip code.\"</p> <p>Agents now return structured data:</p> <pre><code>{\n  \"action\": \"retrieve\",\n  \"status\": \"SUCCESS\",\n  \"results\": [\n    {\n      \"name\": \"Niagara Falls International Airport\",\n      \"state\": \"New York\",\n      \"zip_code\": \"14304\"\n    },\n    {\n      \"name\": \"Buffalo-Niagara International Airport\",\n      \"state\": \"New York\",\n      \"zip_code\": \"14225\"\n    }\n  ]\n}\n</code></pre>"},{"location":"changelog/v1.0.0-rc.1/#benefits_1","title":"Benefits","text":"<ul> <li>Eliminates format ambiguity: Clear specifications for how to structure results</li> <li>Consistent evaluation: All agents use the same format, enabling reliable comparisons</li> <li>Structured data validation: Results can be validated against JSON schemas</li> </ul>"},{"location":"evaluation/","title":"Evaluation","text":"<p>WebArena-Verified provides a comprehensive evaluation framework for assessing web agent performance. The system validates agent behavior through multiple evaluators that check different aspects of task completion.</p>"},{"location":"evaluation/#evaluator-configuration","title":"Evaluator Configuration","text":"<p>Each task defines its validation requirements through evaluator configurations:</p> <ul> <li>One agent response evaluator - Every task has exactly one <code>AgentResponseEvaluator</code> configuration that validates the agent's final structured response (performed operation, status, and retrieved data).</li> <li>Zero or more network event evaluators - Depending on the expected operation, a task may include zero to multiple <code>NetworkEventEvaluator</code> configurations. Navigate and mutate operations typically require network validation, while retrieve operations may not need any network checks.</li> </ul>"},{"location":"evaluation/#evaluation-method-comparison","title":"Evaluation Method Comparison","text":"Aspect WebArena WebArena-Verified Validation Approach DOM-based evaluation Network event-based evaluation Matching Method Substring matching and LLM-as-judge eval Data type-aware exact match LLM-Based Evaluation LLM-based evaluation Replaced by exact match Stability Fragile - breaks with UI changes Stable - resilient to UI changes Tool Dependency Tightly coupled to specific frameworks Framework-flexible (any tool with network traces) Offline Evaluation Not supported Supported - re-evaluate captured traces"},{"location":"evaluation/#learn-more","title":"Learn More","text":"<ul> <li>Evaluation Results - Complete guide to understanding evaluation output format and results</li> <li>Network Event-Based Evaluation - Detailed guide on network trace validation using HTTP Archive (HAR) format</li> <li>Removing LLM-Based Evaluation - How we replaced LLM-as-judge with exact matching and verifiable intents</li> <li>Handling of Unachievable Tasks - Guidance on replacing N/A with explicit statuses and reducing guesswork</li> </ul>"},{"location":"evaluation/evaluation_results/","title":"Evaluation Results","text":"<p>This guide explains the evaluation result format produced by WebArena-Verified's evaluation system.</p>"},{"location":"evaluation/evaluation_results/#overview","title":"Overview","text":"<p>When you evaluate tasks, WebArena-Verified generates structured JSON files containing:</p> <ul> <li>Task metadata and identification</li> <li>Overall score and status</li> <li>Individual evaluator results</li> <li>Version and checksum information for reproducibility</li> </ul> <p>Result files are saved as <code>eval_result.json</code> in each task's output directory.</p>"},{"location":"evaluation/evaluation_results/#result-file-format","title":"Result File Format","text":""},{"location":"evaluation/evaluation_results/#complete-example","title":"Complete Example","text":"<p>Here's a complete evaluation result from a successful task:</p> <pre><code>{\n  \"task_id\": 676,\n  \"intent_template_id\": 253,\n  \"sites\": [\"shopping_admin\"],\n  \"task_revision\": 2,\n  \"status\": \"success\",\n  \"score\": 1.0,\n  \"evaluators_results\": [\n    {\n      \"evaluator_name\": \"AgentResponseEvaluator\",\n      \"status\": \"success\",\n      \"score\": 1.0,\n      \"actual\": {\n        \"action\": \"navigate\",\n        \"status\": \"SUCCESS\",\n        \"results\": null,\n        \"error_details\": null\n      },\n      \"actual_normalized\": {\n        \"action\": \"navigate\",\n        \"status\": \"SUCCESS\",\n        \"results\": null,\n        \"error_details\": null\n      },\n      \"expected\": {\n        \"action\": \"navigate\",\n        \"status\": \"SUCCESS\",\n        \"results\": null,\n        \"error_details\": null\n      },\n      \"assertions\": null,\n      \"error_msg\": null\n    },\n    {\n      \"evaluator_name\": \"NetworkEventEvaluator\",\n      \"status\": \"success\",\n      \"score\": 1.0,\n      \"actual\": {\n        \"url\": \"__shopping_admin__/sales/order/\",\n        \"headers\": {\n          \"referer\": \"__shopping_admin__/dashboard/\"\n        },\n        \"response_status\": 200,\n        \"query_string\": {},\n        \"post_data\": {},\n        \"event_type\": \"navigation\",\n        \"http_method\": \"GET\"\n      },\n      \"actual_normalized\": {\n        \"url\": \"__shopping_admin__/sales/order/\",\n        \"headers\": {\n          \"referer\": \"__shopping_admin__/dashboard/\"\n        },\n        \"response_status\": 200,\n        \"query_string\": {},\n        \"post_data\": {},\n        \"event_type\": \"navigation\",\n        \"http_method\": \"GET\"\n      },\n      \"expected\": {\n        \"url\": \"__shopping_admin__/sales/order/\",\n        \"headers\": {\n          \"referer\": \"__shopping_admin__/dashboard/\"\n        },\n        \"response_status\": 200,\n        \"query_string\": {},\n        \"post_data\": {},\n        \"event_type\": \"navigation\",\n        \"http_method\": \"GET\"\n      },\n      \"assertions\": null,\n      \"error_msg\": null\n    }\n  ],\n  \"error_msg\": null,\n  \"webarena_verified_version\": \"1.0.0-rc.1\",\n  \"webarena_verified_evaluator_checksum\": \"27e007a063d15058672f721653068f7abd4c0b85556b5000c2e555f39a3db422\",\n  \"webarena_verified_data_checksum\": \"035da5132fe32c25ed12c1fdb012fe55749202dca1eb0dc183e9ab7043f76984\"\n}\n</code></pre>"},{"location":"evaluation/evaluation_results/#top-level-fields","title":"Top-Level Fields","text":""},{"location":"evaluation/evaluation_results/#task-identification","title":"Task Identification","text":"Field Type Description <code>task_id</code> <code>int</code> Unique identifier for the task <code>intent_template_id</code> <code>int</code> Groups tasks generated from the same template <code>sites</code> <code>array[string]</code> List of platforms involved (e.g., <code>[\"shopping_admin\"]</code>, <code>[\"gitlab\", \"reddit\"]</code>) <code>task_revision</code> <code>int</code> Version number of the task definition (increments when task is updated)"},{"location":"evaluation/evaluation_results/#evaluation-results_1","title":"Evaluation Results","text":"Field Type Description <code>status</code> <code>string</code> Overall evaluation status: <code>\"success\"</code>, <code>\"failure\"</code>, <code>\"partial_match\"</code>, or <code>\"error\"</code> <code>score</code> <code>float</code> Overall score between 0.0 and 1.0. Score is 1.0 only if all evaluators succeed <code>evaluators_results</code> <code>array</code> Results from each individual evaluator (see Evaluator Results) <code>error_msg</code> <code>string\\|null</code> Error message if evaluation failed, otherwise <code>null</code>"},{"location":"evaluation/evaluation_results/#version-tracking","title":"Version Tracking","text":"Field Type Description <code>webarena_verified_version</code> <code>string</code> Version of WebArena-Verified that performed the evaluation <code>webarena_verified_evaluator_checksum</code> <code>string</code> SHA-256 checksum of evaluator code (detects evaluator changes) <code>webarena_verified_data_checksum</code> <code>string</code> SHA-256 checksum of dataset file (detects task definition changes) <p>Version tracking ensures reproducibility - you can detect if evaluation results differ due to code changes, dataset updates, or actual agent performance differences.</p>"},{"location":"evaluation/evaluation_results/#evaluator-results","title":"Evaluator Results","text":"<p>Each entry in <code>evaluators_results</code> represents the output of a single evaluator. Multiple evaluators can run for a single task to validate different aspects.</p>"},{"location":"evaluation/evaluation_results/#evaluator-result-structure","title":"Evaluator Result Structure","text":"<pre><code>{\n  \"evaluator_name\": \"AgentResponseEvaluator\",\n  \"status\": \"success\",\n  \"score\": 1.0,\n  \"actual\": { ... },\n  \"actual_normalized\": { ... },\n  \"expected\": { ... },\n  \"assertions\": null,\n  \"error_msg\": null\n}\n</code></pre>"},{"location":"evaluation/evaluation_results/#evaluator-result-fields","title":"Evaluator Result Fields","text":"Field Type Description <code>evaluator_name</code> <code>string</code> Name of the evaluator that ran (e.g., <code>\"AgentResponseEvaluator\"</code>, <code>\"NetworkEventEvaluator\"</code>) <code>status</code> <code>string</code> Evaluator status: <code>\"success\"</code>, <code>\"failure\"</code>, or <code>\"error\"</code> <code>score</code> <code>float</code> Evaluator score between 0.0 and 1.0 <code>actual</code> <code>object\\|null</code> What the agent actually produced (raw format) <code>actual_normalized</code> <code>object\\|null</code> Normalized version of actual output (for comparison) <code>expected</code> <code>object\\|null</code> What was expected (from task definition) <code>assertions</code> <code>array\\|null</code> Detailed assertion results if available (see Assertions) <code>error_msg</code> <code>string\\|null</code> Error message if evaluator failed, otherwise <code>null</code>"},{"location":"evaluation/evaluation_results/#common-evaluators","title":"Common Evaluators","text":""},{"location":"evaluation/evaluation_results/#agentresponseevaluator","title":"AgentResponseEvaluator","text":"<p>Validates the agent's structured response format, action type, status, and results.</p> <p>Note: This evaluator examines <code>TaskEvalContext.agent_response_raw</code> to validate the agent's response.</p> <p>Example Output:</p> <pre><code>{\n  \"evaluator_name\": \"AgentResponseEvaluator\",\n  \"status\": \"success\",\n  \"score\": 1.0,\n  \"actual\": {\n    \"action\": \"retrieve\",\n    \"status\": \"SUCCESS\",\n    \"results\": [\"Product A\", \"Product B\"],\n    \"error_details\": null\n  },\n  \"expected\": {\n    \"action\": \"retrieve\",\n    \"status\": \"SUCCESS\",\n    \"results\": [\"Product A\", \"Product B\"],\n    \"error_details\": null\n  }\n}\n</code></pre>"},{"location":"evaluation/evaluation_results/#networkeventevaluator","title":"NetworkEventEvaluator","text":"<p>Validates navigation and network requests by matching URLs, headers, query parameters, and status codes captured in HAR traces.</p> <p>Note: This evaluator examines <code>TaskEvalContext.network_trace</code> to validate network events.</p> <p>Example Output:</p> <pre><code>{\n  \"evaluator_name\": \"NetworkEventEvaluator\",\n  \"status\": \"success\",\n  \"score\": 1.0,\n  \"actual\": {\n    \"url\": \"__shopping_admin__/sales/order/\",\n    \"headers\": {\n      \"referer\": \"__shopping_admin__/dashboard/\"\n    },\n    \"response_status\": 200,\n    \"query_string\": {},\n    \"post_data\": {},\n    \"event_type\": \"navigation\",\n    \"http_method\": \"GET\"\n  },\n  \"expected\": {\n    \"url\": \"__shopping_admin__/sales/order/\",\n    \"headers\": {\n      \"referer\": \"__shopping_admin__/dashboard/\"\n    },\n    \"response_status\": 200,\n    \"query_string\": {},\n    \"post_data\": {},\n    \"event_type\": \"navigation\",\n    \"http_method\": \"GET\"\n  }\n}\n</code></pre>"},{"location":"evaluation/evaluation_results/#understanding-status-values","title":"Understanding Status Values","text":""},{"location":"evaluation/evaluation_results/#task-level-status","title":"Task-Level Status","text":"Status Description Score When It Occurs <code>success</code> Task completed successfully <code>1.0</code> All evaluators have status <code>\"success\"</code> and score <code>1.0</code> <code>failure</code> Task failed validation <code>0.0</code> One or more evaluators have status <code>\"failure\"</code> <code>error</code> Evaluation encountered an error <code>0.0</code> One or more evaluators encountered an error during execution"},{"location":"evaluation/evaluation_results/#evaluator-level-status","title":"Evaluator-Level Status","text":"<p>Each evaluator can have one of these statuses:</p> <ul> <li><code>success</code>: Evaluator validation passed completely</li> <li><code>failure</code>: Evaluator validation failed (actual didn't match expected)</li> <li><code>error</code>: Evaluator encountered an error during execution (e.g., missing files, malformed data)</li> </ul>"},{"location":"evaluation/evaluation_results/#assertions","title":"Assertions","text":"<p>Some evaluators provide detailed assertion-level results to explain why validation succeeded or failed.</p>"},{"location":"evaluation/evaluation_results/#assertion-structure","title":"Assertion Structure","text":"<pre><code>{\n  \"assertion_name\": \"url_match\",\n  \"status\": \"success\",\n  \"assertion_msgs\": [\n    \"URL matched expected pattern\"\n  ],\n  \"error_msg\": null\n}\n</code></pre>"},{"location":"evaluation/evaluation_results/#assertion-fields","title":"Assertion Fields","text":"Field Type Description <code>assertion_name</code> <code>string</code> Name identifying this specific assertion <code>status</code> <code>string</code> Assertion result: <code>\"success\"</code>, <code>\"failure\"</code>, or <code>\"error\"</code> <code>assertion_msgs</code> <code>array[string]\\|null</code> Human-readable messages explaining the assertion result <code>error_msg</code> <code>string\\|null</code> Error message if assertion encountered an error"},{"location":"evaluation/evaluation_results/#common-result-patterns","title":"Common Result Patterns","text":""},{"location":"evaluation/evaluation_results/#successful-evaluation","title":"Successful Evaluation","text":"<p>All evaluators pass:</p> <pre><code>    {\n      \"status\": \"success\",\n      \"score\": 1.0,\n      \"evaluators_results\": [\n        {\n          \"evaluator_name\": \"AgentResponseEvaluator\",\n          \"status\": \"success\",\n          \"score\": 1.0\n        },\n        {\n          \"evaluator_name\": \"NetworkEventEvaluator\",\n          \"status\": \"success\",\n          \"score\": 1.0\n        }\n      ],\n      \"error_msg\": null\n}\n</code></pre>"},{"location":"evaluation/evaluation_results/#failed-evaluation","title":"Failed Evaluation","text":"<p>One or more evaluators fail:</p> <pre><code>    {\n      \"status\": \"failure\",\n      \"score\": 0.0,\n      \"evaluators_results\": [\n        {\n          \"evaluator_name\": \"AgentResponseEvaluator\",\n          \"status\": \"success\",\n          \"score\": 1.0\n        },\n        {\n          \"evaluator_name\": \"NetworkEventEvaluator\",\n          \"status\": \"failure\",\n          \"score\": 0.0,\n          \"error_msg\": \"No network events matched criteria: {'url': '__shopping_admin__/sales/order/'}\"\n        }\n      ],\n      \"error_msg\": null\n    }\n  ```\n\n### Evaluation Error\n\nEvaluator encountered an error during execution:\n\n```json\n{\n  \"status\": \"error\",\n  \"score\": 0.0,\n  \"evaluators_results\": [\n    {\n      \"evaluator_name\": \"AgentResponseEvaluator\",\n      \"status\": \"error\",\n      \"score\": 0.0,\n      \"error_msg\": \"Failed to parse agent_response.json: Expecting property name enclosed in double quotes\"\n    }\n  ],\n  \"error_msg\": \"One or more evaluators encountered errors\"\n}\n</code></pre>"},{"location":"evaluation/evaluation_results/#batch-evaluation-results","title":"Batch Evaluation Results","text":"<p>When evaluating multiple tasks using <code>eval-tasks</code>, WebArena-Verified generates:</p> <ol> <li>Individual task results: One <code>task_{id}_eval_result.json</code> per task in each task's directory</li> <li>Summary file: <code>eval_summary.json</code> in the output directory with aggregated statistics</li> </ol>"},{"location":"evaluation/evaluation_results/#summary-file-format","title":"Summary File Format","text":"<pre><code>{\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"webarena_verified_version\": \"1.0.0-rc.1\",\n  \"webarena_verified_evaluator_checksum\": \"27e007a...\",\n  \"webarena_verified_data_checksum\": \"035da51...\",\n  \"total\": 100,\n  \"success_count\": 87,\n  \"failed_count\": 12,\n  \"error_count\": 1,\n  \"per_site_summary\": {\n    \"shopping_admin\": [\n      {\"task_id\": 1, \"status\": \"success\", \"score\": 1.0},\n      {\"task_id\": 2, \"status\": \"success\", \"score\": 1.0}\n    ],\n    \"gitlab\": [\n      {\"task_id\": 50, \"status\": \"failure\", \"score\": 0.0}\n    ]\n  },\n  \"task_results\": [ ... ]\n}\n</code></pre>"},{"location":"evaluation/evaluation_results/#using-evaluation-results","title":"Using Evaluation Results","text":""},{"location":"evaluation/evaluation_results/#programmatic-access","title":"Programmatic Access","text":"<p>Read and analyze results in Python:</p> <pre><code>import json\nfrom pathlib import Path\n\n# Load result file\nresult_path = Path(\"output/task_1/task_1_eval_result.json\")\nwith open(result_path) as f:\n    result = json.load(f)\n\n# Check if task passed\nif result[\"status\"] == \"success\" and result[\"score\"] == 1.0:\n    print(f\"Task {result['task_id']} passed!\")\nelse:\n    print(f\"Task {result['task_id']} failed\")\n    for eval_result in result[\"evaluators_results\"]:\n        if eval_result[\"status\"] != \"success\":\n            print(f\"  - {eval_result['evaluator_name']}: {eval_result['error_msg']}\")\n</code></pre>"},{"location":"evaluation/evaluation_results/#filtering-results","title":"Filtering Results","text":"<p>Find all failed tasks:</p> <pre><code># Using jq\nfind output -name \"*_eval_result.json\" -exec jq -r 'select(.status != \"success\") | .task_id' {} \\;\n</code></pre>"},{"location":"evaluation/evaluation_results/#computing-pass-rate","title":"Computing Pass Rate","text":"<pre><code># Count successes vs total\ntotal=$(find output -name \"*_eval_result.json\" | wc -l)\npassed=$(find output -name \"*_eval_result.json\" -exec jq -r 'select(.status == \"success\") | .task_id' {} \\; | wc -l)\necho \"Pass rate: $passed / $total\"\n</code></pre>"},{"location":"evaluation/evaluation_results/#troubleshooting","title":"Troubleshooting","text":""},{"location":"evaluation/evaluation_results/#score-is-00-but-status-is-success","title":"Score is 0.0 but status is \"success\"","text":"<p>This shouldn't happen in WebArena-Verified. If you encounter this, it may indicate:</p> <ul> <li>A bug in evaluator logic</li> <li>Corrupted result file</li> </ul> <p>Please report this as an issue.</p>"},{"location":"evaluation/evaluation_results/#missing-evaluators_results","title":"Missing evaluators_results","text":"<p>If <code>evaluators_results</code> is empty or missing, check:</p> <ul> <li>The task definition has valid evaluator configurations</li> <li>All required files are present (agent_response.json, trace.zip)</li> <li>The evaluation didn't encounter an early error (check <code>error_msg</code>)</li> </ul>"},{"location":"evaluation/evaluation_results/#checksum-mismatches","title":"Checksum Mismatches","text":"<p>If you re-evaluate the same task and get different checksums:</p> <ul> <li>evaluator_checksum changed: Evaluator code was modified (code update, bug fix)</li> <li>data_checksum changed: Task definition was updated (dataset version change)</li> </ul> <p>Different checksums don't necessarily mean results are invalid, but they indicate the evaluation conditions changed.</p>"},{"location":"evaluation/evaluation_results/#see-also","title":"See Also","text":"<ul> <li>Network Event Based Evaluation - Deep dive into network trace validation</li> <li>Getting Started Guide - Learn how to run evaluations</li> <li>API Reference: EvaluatorResult - Technical schema details</li> </ul>"},{"location":"evaluation/handling_of_unachievable_tasks/","title":"Handling of Unachievable Tasks","text":"<p>This page explains how we handle action outcome statuses in evaluation, replacing catch\u2011all labels with explicit, diagnosable statuses to improve determinism and reduce guesswork.</p>"},{"location":"evaluation/handling_of_unachievable_tasks/#replacing-catchall-na","title":"Replacing Catch\u2011All N/A","text":"<p>Previously, some unachievable tasks were labeled with a generic <code>N/A</code>. We now require returning a specific status that describes the failure mode instead of <code>N/A</code>.</p> <ul> <li>Use one of the explicit error codes (e.g., <code>NOT_FOUND_ERROR</code>, <code>ACTION_NOT_ALLOWED_ERROR</code>, <code>PERMISSION_DENIED_ERROR</code>, <code>DATA_VALIDATION_ERROR</code>, <code>UNKNOWN_ERROR</code>) that best fits the situation.</li> <li>Benefits: Clearer evaluation, reproducible exact matching, and easier debugging of failure causes.</li> <li>Reference: See the canonical list in <code>docs/api_reference/data_types/agent_response.md</code>.</li> </ul> <p>The AI agent selects the status code at runtime based on its assessment of the failure mode. The framework does not pre\u2011select or auto\u2011map statuses.</p>"},{"location":"evaluation/handling_of_unachievable_tasks/#response-requirements","title":"Response Requirements","text":"<ul> <li>Keep <code>action</code> consistent with the intended operation (<code>retrieve</code>, <code>navigate</code>, <code>mutate</code>).</li> <li>For failure statuses, set <code>results</code> to <code>null</code> (or <code>[]</code> only when explicitly allowed).</li> <li><code>error_details</code> is optional and ignored for scoring; include brief context for debugging.</li> </ul>"},{"location":"evaluation/handling_of_unachievable_tasks/#examples","title":"Examples","text":"Retrieve \u2014 not found <pre><code>{\n  \"action\": \"retrieve\",\n  \"status\": \"NOT_FOUND_ERROR\",\n  \"results\": null,\n  \"error_details\": \"No invoices found for 2021-01 through 2021-03.\"\n}\n</code></pre> Mutate \u2014 permission denied <pre><code>{\n  \"action\": \"mutate\",\n  \"status\": \"PERMISSION_DENIED_ERROR\",\n  \"results\": null,\n  \"error_details\": \"User lacks admin role to delete project.\"\n}\n</code></pre> Navigate \u2014 action not allowed <pre><code>{\n  \"action\": \"navigate\",\n  \"status\": \"ACTION_NOT_ALLOWED_ERROR\",\n  \"results\": null,\n  \"error_details\": \"Direct navigation to billing page is disabled for this role.\"\n}\n</code></pre>"},{"location":"evaluation/handling_of_unachievable_tasks/#evaluator-expectations","title":"Evaluator Expectations","text":"<ul> <li>Scoring uses exact match on <code>status</code> and basic structural checks (e.g., <code>results</code> is <code>null</code> for failures).</li> <li><code>error_details</code> does not affect scoring; it\u2019s for analysis only.</li> <li>For tasks labeled unachievable in the dataset, returning the expected specific status is required; generic <code>N/A</code> is not accepted.</li> </ul>"},{"location":"evaluation/handling_of_unachievable_tasks/#fewer-free-passes-less-guessing","title":"Fewer Free Passes, Less Guessing","text":"<p>In the original harness, a catch\u2011all \u201cN/A\u201d could be credited even after minimal exploration, creating asymmetric grading: high recall but low precision for failures. By requiring explicit, agent\u2011selected status codes, premature \u201cN/A\u201d answers are no longer auto\u2011credited.</p> <ul> <li>Effect on spurious passes: The evaluator expects an exact, task\u2011appropriate status; superficial attempts tend to pick incorrect labels and fail.</li> <li>Effect on guessing: Without a catch\u2011all, naive guessing among multiple failure labels is unlikely to match the expected status (intuitively ~1/M when M valid failure codes exist).</li> <li>Exploration adequacy: The protocol favors agents that explore sufficiently before concluding failure, aligning outcomes with the paper\u2019s analysis of under\u2011explored \u201cN/A\u201d cases.</li> </ul> <p>Takeaway: Removing <code>N/A</code> and requiring explicit statuses reduces accidental credits, encourages adequate exploration, and yields more reliable failure reporting.</p>"},{"location":"evaluation/network_event_based_evaluation/","title":"Network Event-Based Evaluation","text":"<p>WebArena-Verified evaluates web agents by analyzing network events extracted from HTTP Archive (HAR) format traces captured during task execution. This approach validates agent behavior at the network level\u2014examining navigation URLs, HTTP Referer headers, and status codes\u2014rather than relying on fragile DOM-based assertions. By focusing on the actual HTTP interactions that occur when agents browse the web, WebArena-Verified provides stable, maintainable evaluation that is resilient to UI changes.</p>"},{"location":"evaluation/network_event_based_evaluation/#what-are-har-files","title":"What are HAR Files?","text":"<p>HAR (HTTP Archive) is a JSON-based format that logs all HTTP requests and responses made by a web browser during a session. HAR files capture:</p> <ul> <li>Request URLs and HTTP methods (GET, POST, etc.)</li> <li>Request and response headers (including HTTP Referer headers and content types)</li> <li>HTTP status codes (200 OK, 404 Not Found, etc.)</li> <li>Response bodies and timing information</li> <li>Network redirects and navigation events</li> </ul> <p>In WebArena-Verified, we extract network traces from browser automation tools (like Playwright) and convert them to a structured format for validation.</p>"},{"location":"evaluation/network_event_based_evaluation/#comparison-of-evaluation-methods","title":"Comparison of Evaluation Methods","text":"<p>WebArena-Verified uses network trace validation instead of traditional DOM-based approaches. This shift provides significant advantages in stability and maintainability.</p>"},{"location":"evaluation/network_event_based_evaluation/#previous-method-dom-based-evaluation","title":"Previous Method: DOM-Based Evaluation","text":"<p>Traditional web agent evaluation relied on DOM interaction through browser automation:</p> <ul> <li>Using CSS or XPath selectors to find HTML elements</li> <li>Verifying agent actions by checking element properties and visibility</li> <li>Monitoring DOM changes to confirm user interactions</li> </ul> <p>Issues with the DOM-Based Approach:</p> <ul> <li>Fragility: Tests break when page layout or element attributes change</li> <li>High Maintenance: Requires frequent selector updates as UIs evolve</li> <li>Limited Scope: Focuses on rendered DOM rather than actual network interactions</li> <li>Tool Lock-in: Tightly coupled to specific automation frameworks</li> </ul>"},{"location":"evaluation/network_event_based_evaluation/#current-method-network-trace-validation","title":"Current Method: Network Trace Validation","text":"<p>WebArena-Verified validates agent behavior by analyzing network traces captured during task execution. The system:</p> <ul> <li>Extracts navigation events from browser network logs</li> <li>Validates URLs, referers, and HTTP status codes</li> <li>Compares actual navigation patterns against expected outcomes</li> <li>Supports flexible URL matching with query parameter normalization</li> </ul> <p>Benefits of Network Trace Validation:</p> <ul> <li>Stability: Resilient to UI changes since validation occurs at the network level</li> <li>Reduced Maintenance: No selector updates needed when UI changes</li> <li>Framework Flexibility: Works with any tool that exports network traces (Playwright, Puppeteer, etc.)</li> <li>Offline Evaluation: Previously captured traces can be re-evaluated without browser replay</li> <li>Better Signal: Validates the actual HTTP interactions rather than rendered appearance</li> </ul>"},{"location":"evaluation/network_event_based_evaluation/#what-gets-validated","title":"What Gets Validated","text":"<p>Network trace validation is powered by the <code>NetworkEventEvaluator</code>, the only evaluator included with the open-source build for inspecting traces. It normalizes captured events and asserts that the agent triggered the right HTTP traffic.</p>"},{"location":"evaluation/network_event_based_evaluation/#1-navigation-requests","title":"1. Navigation Requests","text":"<p>You can ensure the agent reached a specific page by checking the final navigation event:</p> <pre><code>{\n  \"evaluator\": \"NetworkEventEvaluator\",\n  \"last_event_only\": true,\n  \"expected\": {\n    \"url\": \"__SHOPPING__/products/123\",\n    \"response_status\": 200\n  }\n}\n</code></pre>"},{"location":"evaluation/network_event_based_evaluation/#2-query-parameters-and-post-data","title":"2. Query Parameters and POST Data","text":"<p>The evaluator can assert normalized query strings or form bodies. Use <code>ignored_query_params</code> to drop volatile keys, and the optional schema helpers for type-aware comparisons:</p> <pre><code>{\n  \"evaluator\": \"NetworkEventEvaluator\",\n  \"ignored_query_params\": [\"session_id\", \"timestamp\"],\n  \"query_params_schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"from\": {\"type\": \"string\", \"format\": \"date\"},\n      \"to\": {\"type\": \"string\", \"format\": \"date\"}\n    }\n  },\n  \"expected\": {\n    \"url\": \"__SHOPPING__/reports/sales\",\n    \"query_params\": {\n      \"from\": [\"02/01/2023\"],\n      \"to\": [\"02/28/2023\"]\n    }\n  }\n}\n</code></pre>"},{"location":"evaluation/network_event_based_evaluation/#3-headers-and-referers","title":"3. Headers and Referers","text":"<p>Because HAR captures request headers, the evaluator can confirm a navigation originated from the right page:</p> <pre><code>{\n  \"evaluator\": \"NetworkEventEvaluator\",\n  \"expected\": {\n    \"url\": \"__SHOPPING__/checkout\",\n    \"headers\": {\n      \"referer\": \"__SHOPPING__/cart\"\n    }\n  }\n}\n</code></pre>"},{"location":"evaluation/network_event_based_evaluation/#4-event-types-and-sequencing","title":"4. Event Types and Sequencing","text":"<p>Set <code>event_type</code> to <code>\"navigation\"</code> to focus on page loads, or to <code>\"modification\"</code> for form submissions. The <code>last_event_only</code> flag instructs the evaluator to match the most recent event; disabling it means \"any matching event is sufficient\".</p>"},{"location":"evaluation/network_event_based_evaluation/#network-trace-structure","title":"Network Trace Structure","text":"<p>Network traces contain structured navigation events extracted from HAR-format logs:</p> <pre><code>class NetworkEvent:\n    url: str                  # Request URL\n    referer: str | None       # Referer header value\n    http_method: str          # GET, POST, etc.\n    request_status: int       # HTTP status code (200, 404, etc.)\n    is_document_event: bool   # True for document navigations\n    event_type: NetworkEventType  # NAVIGATION, MUTATION, or OTHER\n\nclass NetworkTrace:\n    events: tuple[NetworkEvent, ...]\n    navigation_events: tuple[NetworkEvent, ...]  # Filtered navigation events only\n</code></pre> <p>Navigation events are identified by request headers:</p> <ul> <li><code>sec-fetch-dest: document</code></li> <li><code>sec-fetch-mode: navigate</code></li> <li>HTTP method: GET (navigations) or POST/PUT/DELETE/PATCH (mutations)</li> </ul>"},{"location":"evaluation/network_event_based_evaluation/#example-navigation-validation","title":"Example: Navigation Validation","text":"<p>Here's a complete example showing how validation works:</p> <p>Task: Agent should navigate to the product detail page for item 123</p> <p>Captured navigation events:</p> <pre><code>[\n    NetworkEvent(url=\"http://shop.test/home\", http_method=\"GET\", request_status=200),\n    NetworkEvent(url=\"http://shop.test/search?q=item\", http_method=\"GET\", request_status=200),\n    NetworkEvent(url=\"http://shop.test/products/123\", http_method=\"GET\", request_status=200)\n]\n</code></pre> <p>Evaluation config:</p> <pre><code>{\n  \"evaluator\": \"NetworkEventEvaluator\",\n  \"last_event_only\": true,\n  \"expected\": {\n    \"url\": \"http://shop.test/products/123\",\n    \"response_status\": 200,\n    \"headers\": {\n      \"referer\": \"http://shop.test/search\"\n    }\n  }\n}\n</code></pre> <p>Result: PASS \u2013 The last navigation event matches the expected URL and referer with status 200</p>"},{"location":"evaluation/network_event_based_evaluation/#limitations","title":"Limitations","text":"<p>While network trace validation provides significant advantages, it has some constraints:</p>"},{"location":"evaluation/network_event_based_evaluation/#what-network-traces-cannot-validate","title":"What Network Traces Cannot Validate","text":"<ol> <li>Visual UI State: Cannot verify rendered appearance, colors, fonts, or layout</li> <li>DOM Content: Cannot check if specific text appears on the page</li> <li>JavaScript State: Cannot inspect client-side application state</li> <li>Dynamic Content: Cannot verify content loaded after initial page render</li> <li>User Experience: Cannot validate animations, transitions, or interactivity</li> </ol>"},{"location":"evaluation/network_event_based_evaluation/#when-to-use-additional-evaluators","title":"When to Use Additional Evaluators","text":"<p>For comprehensive validation, pair network trace validation with:</p> <ul> <li>AgentResponseEvaluator: Ensures the agent returns the expected structured response payload.</li> </ul>"},{"location":"evaluation/network_event_based_evaluation/#technical-constraints","title":"Technical Constraints","text":"<ul> <li>Requires structured traces: The system expects navigation events in a specific format, not raw HAR JSON</li> <li>Capture required: Initial task execution requires a live browser to capture network traces</li> <li>Tool integration: While HAR is standard, extracting navigation events requires integration with browser automation tools</li> </ul>"},{"location":"evaluation/removing_llm_based_evaluation/","title":"Removing LLM-Based Evaluation","text":"<p>WebArena-Verified removed LLM-as-judge evaluation in favor of deterministic, data type-aware exact matching. This improves stability and reproducibility.</p>"},{"location":"evaluation/removing_llm_based_evaluation/#what-changed","title":"What Changed","text":"<ul> <li>Replaced subjective LLM judgments with exact-match checks over structured outputs.</li> <li>Introduced two complementary strategies to make outputs verifiable without an LLM:   1) Explicit format specification   2) Intent phrasing that yields verifiable data</li> </ul>"},{"location":"evaluation/removing_llm_based_evaluation/#1-explicit-format-specification","title":"1) Explicit Format Specification","text":"<p>Format specifications describe the expected output structure when the task can return structured data. These are stored in <code>instantiation_dict</code> as <code>retrieved_data_format_spec</code> and are integrated into the <code>intent_template</code>, making them part of the natural task description that agents see.</p> <p>Guidelines:</p> <ul> <li>Include format requirements directly in the <code>intent</code> via the template.</li> <li>Store format specification in <code>instantiation_dict[\"retrieved_data_format_spec\"]</code> for tasks instantiated from templates.</li> <li>Reference it in <code>intent_template</code> using <code>{{retrieved_data_format_spec}}</code>.</li> <li>Validate results by parsing the agent's output according to the spec and performing exact comparisons.</li> </ul> <p>Example (Task 107)</p> <pre><code>{\n  \"sites\": [\"shopping_admin\"],\n  \"task_id\": 107,\n  \"intent_template_id\": 270,\n  \"start_urls\": [\"__SHOPPING_ADMIN__\"],\n  \"intent\": \"Get the monthly count of successful orders from May to December 2022. Return a list of objects, where each object includes a \\\"month\\\" field for the month and a \\\"count\\\" field for the count.\",\n  \"intent_template\": \"Get the monthly count of successful orders from {{start_month}} to {{end_month}} {{year}}. {{retrieved_data_format_spec}}.\",\n  \"instantiation_dict\": {\n    \"start_month\": \"May\",\n    \"end_month\": \"December\",\n    \"year\": 2022,\n    \"retrieved_data_format_spec\": \"Return a list of objects, where each object includes a \\\"month\\\" field for the month and a \\\"count\\\" field for the count\"\n  }\n}\n</code></pre> <p>Rationale</p> <ul> <li>Integrates format requirements naturally into the task description.</li> <li>Makes output objectively checkable with strict, data type-aware exact matching.</li> <li>Enables template reusability with different format specifications for different instantiations.</li> </ul>"},{"location":"evaluation/removing_llm_based_evaluation/#2-make-the-intent-verifiable","title":"2) Make the Intent Verifiable","text":"<p>When a task is too open-ended for a clear schema, rephrase the intent so the answer is directly checkable against ground truth.</p> <p>Example (IG 163)</p> <ul> <li>Before: <code>What are the main criticisms of this product? Please extract the relevant sentences.</code></li> <li>After: <code>List all review titles with 2 stars or below for this product.</code></li> </ul> <p>Rationale</p> <ul> <li>Shifts from subjective summarization to objective retrieval.</li> <li>Lets the evaluator verify via exact matching of known review titles/ratings.</li> </ul>"},{"location":"evaluation/removing_llm_based_evaluation/#why-remove-llm-based-evaluation","title":"Why Remove LLM-Based Evaluation","text":"<ul> <li>Stability: Not sensitive to UI or prompt drift.</li> <li>Determinism: Produces consistent, reproducible outcomes without sampling variance.</li> </ul>"},{"location":"evaluation/removing_llm_based_evaluation/#related-docs","title":"Related Docs","text":"<ul> <li>Evaluation Results</li> </ul>"},{"location":"getting_started/configuration/","title":"Configuration","text":"<p>WebArena-Verified uses a JSON configuration file to:</p> <ul> <li>Specify the dataset location</li> <li>Configure environment URLs for each site</li> <li>Provide authentication credentials</li> <li>Select which environment to use when running tasks</li> </ul>"},{"location":"getting_started/configuration/#example","title":"Example","text":"<pre><code>{\n  \"test_data_file\": \"assets/dataset/webarena-verified.json\",\n  \"environments\": {\n    \"__SHOPPING_ADMIN__\": {\n      \"urls\": [\"http://localhost:7780/admin\"],\n      \"active_url_idx\": 0,\n      \"credentials\": {\n        \"username\": \"admin\",\n        \"password\": \"admin1234\"\n      }\n    },\n    \"__SHOPPING__\": {\n      \"urls\": [\"http://localhost:7780\"],\n      \"credentials\": {\n        \"username\": \"user@example.com\",\n        \"password\": \"password123\"\n      }\n    },\n    \"__REDDIT__\": {\n      \"urls\": [\"http://localhost:9999\"],\n      \"credentials\": {\n        \"username\": \"testuser\",\n        \"password\": \"testpass\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"getting_started/configuration/#see-also","title":"See Also","text":"<ul> <li>API Reference: Configuration - Detailed field descriptions</li> <li>Getting Started Guide</li> <li>Evaluation Guide</li> </ul>"},{"location":"getting_started/data_format/","title":"Data Format","text":""},{"location":"getting_started/data_format/#overview","title":"Overview","text":"<p>We use WebArenaVerifiedTask, a Pydantic <code>BaseModel</code>, to map and interact with task data. Pydantic provides automatic validation, type safety, and seamless JSON serialization/deserialization, ensuring data integrity throughout the benchmark pipeline.</p>"},{"location":"getting_started/data_format/#changes-from-webarena","title":"Changes from WebArena","text":"<p>WebArena-Verified is a verified benchmark introducing a more structured and extensible task format compared to the original WebArena benchmark. This section outlines the key changes.</p>"},{"location":"getting_started/data_format/#high-level-changes","title":"High-Level Changes","text":"Aspect WebArena WebArena-Verified Evaluation Structure Single dict with type strings Array of typed evaluator configs Field Organization Flat structure with runtime configs Separation of task data and runtime concerns Type Safety String-based types Discriminated unions with Pydantic models Extensibility Limited evaluator types Multiple evaluator types with clear interfaces"},{"location":"getting_started/data_format/#field-by-field-mapping","title":"Field-by-Field Mapping","text":"WebArena Field WebArena-Verified Field Rationale <code>start_url</code> (string) <code>start_urls</code> (array) Support multiple starting URLs for complex tasks <code>eval</code> (object) <code>eval</code> (array of objects) Simplify and flatten evaluator structure for extensibility <code>eval.eval_types</code> <code>eval[].evaluator</code> More explicit evaluator identification <code>eval.reference_answers</code> <code>eval[].expected</code> Structured expected values per evaluator type <code>eval.reference_url</code> <code>NetworkEventEvaluator</code> Replaced string matching with network trace validation <code>eval.program_html</code> <code>NetworkEventEvaluator</code> Network traces now cover backend checks <code>require_login</code> (removed) Runtime configuration, not task data <code>storage_state</code> (removed) Runtime configuration (local file path) <code>geolocation</code> (removed) Always null in dataset; removed as redundant <code>require_reset</code> (removed) Always false in dataset; removed as redundant - <code>format_specification</code> New: Specifies output format requirements - <code>start_url_context</code> New: Provides context about starting page - <code>revision</code> New: Integer revision number for task changes (minimum 1)"},{"location":"getting_started/data_format/#evaluation-system-migration","title":"Evaluation System Migration","text":"<p>The evaluation system was restructured to use typed evaluators instead of string-based types:</p>"},{"location":"getting_started/data_format/#webarena-evaluation-types-webarena-verified-evaluators","title":"WebArena Evaluation Types \u2192 WebArena-Verified Evaluators","text":"WebArena <code>eval_types</code> WebArena-Verified Evaluator(s) Purpose <code>string_match</code> <code>AgentResponseEvaluator</code> (action=retrieve) Validates agent's returned response format, action type, and result values <code>program_html</code> (persisted changes) <code>NetworkEventEvaluator</code> Validates backend-side mutations via network traces <code>program_html</code> (not persisted changes) <code>NetworkEventEvaluator</code> Network requests capture transient UI interactions <code>url_match</code> <code>NetworkEventEvaluator</code> + <code>AgentResponseEvaluator</code> (action=navigate) Validates navigation to correct URL using network traces"},{"location":"getting_started/data_format/#expected-agent-response-changes","title":"Expected Agent Response Changes","text":"<p>WebArena-Verified introduces a structured agent response format, replacing the plain string format used in WebArena.</p>"},{"location":"getting_started/data_format/#webarena-agent-response","title":"WebArena Agent Response","text":"<p>Agents returned a plain string containing the answer:</p> <pre><code>Quest Lumaflex\u2122 Band\n</code></pre>"},{"location":"getting_started/data_format/#webarena-verified-agent-response","title":"WebArena-Verified Agent Response","text":"<p>Agents return a structured JSON object with explicit action type, status, and results:</p> <pre><code>{\n  \"action\": \"retrieve\",\n  \"status\": \"SUCCESS\",\n  \"results\": [\"Quest Lumaflex\u2122 Band\"]\n}\n</code></pre>"},{"location":"getting_started/data_format/#benefits","title":"Benefits","text":"<ul> <li>Reduced false negatives: Eliminates evaluation failures due to string parsing ambiguities.</li> <li>Explicit status indication: Agents clearly report whether they succeeded or encountered errors. This is especially useful when an agent reaches the maximum number of iterations for navigation tasks but fails at the right page. Without explicit status, the evaluation would incorrectly pass.</li> <li>Action type tracking: Clear indication of the performed action (<code>retrieve</code>, <code>navigate</code>, or <code>mutate</code>). This is especially beneficial to differentiate between navigate and retrieve tasks. For example, a task might expect the agent to navigate, but the agent thinks it needs to retrieve some value. The agent might fail at retrieving the value but still navigate to the right page. Without the action being explicit, the validation would incorrectly pass.</li> </ul>"},{"location":"getting_started/data_format/#result-format-specification","title":"Result Format Specification","text":"<p>WebArena-Verified introduces the <code>format_specification</code> field to eliminate ambiguity in how agents should format their results.</p>"},{"location":"getting_started/data_format/#problem-in-webarena","title":"Problem in WebArena","text":"<p>Without format specifications, agents had to interpret how to format results, leading to evaluation ambiguities. For example, Task ID 10 asks: \"Tell me the full address of all US international airports that are within a driving distance of 60 km to Niagara Falls\"</p> <p>Different agents interpreted \"full address\" differently, producing varied formats that were difficult to evaluate consistently. One leaderboard agent returned:</p> <pre><code>There is one US international airport within 60 km driving distance of Niagara Falls: Buffalo-Niagara International Airport, Holtz Drive, Town of Cheektowaga, Erie County, New York, 14225, United States.\n</code></pre>"},{"location":"getting_started/data_format/#solution-in-webarena-verified","title":"Solution in WebArena-Verified","text":"<p>The <code>format_specification</code> field explicitly defines the expected result structure. For Task ID 10, the format specification states: \"Use \\\"name\\\" for the name, \\\"state\\\" for the state, and \\\"zip_code\\\" for the zip code.\"</p> <p>Agents now return structured data:</p> <pre><code>{\n  \"action\": \"retrieve\",\n  \"status\": \"SUCCESS\",\n  \"results\": [\n    {\n      \"name\": \"Niagara Falls International Airport\",\n      \"state\": \"New York\",\n      \"zip_code\": \"14304\"\n    },\n    {\n      \"name\": \"Buffalo-Niagara International Airport\",\n      \"state\": \"New York\",\n      \"zip_code\": \"14225\"\n    }\n  ]\n}\n</code></pre>"},{"location":"getting_started/data_format/#benefits_1","title":"Benefits","text":"<ul> <li>Eliminates format ambiguity: Clear specifications for how to structure results</li> <li>Consistent evaluation: All agents use the same format, enabling reliable comparisons</li> <li>Structured data validation: Results can be validated against JSON schemas</li> </ul>"},{"location":"getting_started/data_reader/","title":"Data Reader","text":"<p>The <code>WebArenaVerified</code> facade provides easy access to benchmark tasks:</p> <pre><code>from webarena_verified.api import WebArenaVerified\n\n# Initialize with config\nwa = WebArenaVerified()\n\n# Get all tasks\ntasks = wa.get_tasks()\nprint(len(tasks))  # 812 tasks in the full benchmark\n\n# Get a specific task\ntask = wa.get_task(42)\nprint(task.intent_template_id)\nprint(task.expected_agent_response.task_type)\n</code></pre>"},{"location":"getting_started/environments/","title":"Environments","text":"<p>WebArena-Verified uses Docker containers to provide isolated, reproducible environments for each website in the benchmark. To improve reset time and reduce storage requirements, we provide recipes to create slim images - optimized versions of the original environments while keeping the exact content and functionality.</p>"},{"location":"getting_started/environments/#size-improvements","title":"Size Improvements","text":"<p>Slim images are significantly smaller than their original counterparts:</p> Environment Original Size Slim Size Reduction Shopping Admin 19.9 GB 4.98 GB ~70% smaller Shopping 117 GB 17.8 GB ~85% smaller Reddit 107 GB 19 GB ~82% smaller GitLab 155 GB 34 GB ~78% smaller <p>Benefits:</p> <ul> <li>Smaller storage and memory footprint</li> <li>HTTP header-based authentication bypassing UI login</li> <li>All functionality preserved</li> </ul>"},{"location":"getting_started/environments/#shopping-admin","title":"Shopping Admin","text":""},{"location":"getting_started/environments/#1-create-the-slim-image","title":"1. Create the slim image","text":"<pre><code>cd scripts/environments/shopping_admin\nbash create_slim_image.sh\n</code></pre>"},{"location":"getting_started/environments/#2-run-the-container-with-auto-initialization","title":"2. Run the container with auto-initialization","text":"<pre><code>docker run -d --name admin-slim -p 7780:80 \\\n  -e MAGENTO_BASE_URL=http://localhost:7780 \\\n  shopping_admin_final_0719:slim\n</code></pre>"},{"location":"getting_started/environments/#access-the-admin-auto-login-via-header-in-playwright","title":"Access the admin (auto-login via header in Playwright)","text":"<pre><code>from playwright.async_api import async_playwright\n\nasync with async_playwright() as p:\n    browser = await p.chromium.launch()\n    context = await browser.new_context()\n\n    # Set auto-login header\n    await context.set_extra_http_headers({\n        \"X-M2-Admin-Auto-Login\": \"admin:admin1234\"\n    })\n\n    page = await context.new_page()\n    await page.goto(\"http://localhost:7780/admin\")\n    # You're now logged in as admin\n</code></pre>"},{"location":"getting_started/environments/#manual-initialization","title":"Manual Initialization","text":"<p>If you want to manually initialize the environment (e.g., after stopping the container), run:</p> <pre><code># Start container\ndocker run -d --name admin-slim -p 7780:80 shopping_admin_final_0719:slim\n\n# Initialize Magento\ndocker exec admin-slim magento-init http://localhost:7780\n</code></pre>"},{"location":"getting_started/environments/#troubleshooting","title":"Troubleshooting","text":"<p>Auto-login not working:</p> <ol> <li> <p>Test with curl:    <pre><code># Should redirect to dashboard and set cookies\ncurl -I -H \"X-M2-Admin-Auto-Login: admin:admin1234\" \\\n  http://localhost:7780/admin\n\n# Without header - should redirect to login page\ncurl -I http://localhost:7780/admin\n</code></pre></p> </li> <li> <p>Check module is enabled:    <pre><code>docker exec admin-slim /var/www/magento2/bin/magento module:status WebArena_AutoLogin\n</code></pre>    Should show \"Module is enabled\"</p> </li> <li> <p>Check DI is compiled:    <pre><code>docker exec admin-slim ls -la /var/www/magento2/generated/code/\n</code></pre>    Should contain compiled classes</p> </li> <li> <p>Check logs:    <pre><code>docker exec admin-slim tail -f /var/www/magento2/var/log/system.log\n</code></pre>    Look for \"Auto-login successful\" or error messages</p> </li> <li> <p>Recompile if needed:    <pre><code>docker exec admin-slim /var/www/magento2/bin/magento setup:di:compile\n</code></pre></p> </li> </ol> <p>Container not initializing: - Check container logs: <code>docker logs admin-slim</code> - Verify MAGENTO_BASE_URL is set correctly - Wait ~30 seconds for initialization to complete</p> <p>Database reset not working: - Ensure archive exists: <code>docker exec admin-slim ls -lh /var/backups/mysql/data.tar.gz</code> - Check services status: <code>docker exec admin-slim supervisorctl status</code></p>"},{"location":"getting_started/environments/#reddit","title":"Reddit","text":""},{"location":"getting_started/environments/#1-create-the-slim-image_1","title":"1. Create the slim image","text":"<pre><code>cd scripts/environments/reddit\nbash create_slim_image.sh\n</code></pre> <p>Note: The script reuses existing data if available: - PostgreSQL archive (~1.6GB) - Optimized submission images (~6GB) - This saves ~30-60 minutes on subsequent runs</p>"},{"location":"getting_started/environments/#2-run-the-container","title":"2. Run the container","text":"<pre><code>docker run -d --name reddit-slim -p 9999:80 postmill-populated-exposed-withimg:slim\n</code></pre> <p>The container is self-contained (no volume mounts needed) and auto-initializes on first start (~2-3 minutes).</p>"},{"location":"getting_started/environments/#manual-initialization_1","title":"Manual Initialization","text":"<p>Not typically needed (container auto-initializes), but can be run manually if needed:</p> <pre><code># Start container\ndocker run -d --name reddit-slim -p 9999:80 postmill-populated-exposed-withimg:slim\n\n# Check initialization status\ndocker exec reddit-slim postmill-init\n</code></pre>"},{"location":"getting_started/environments/#troubleshooting_1","title":"Troubleshooting","text":"<p>Container not initializing: - Check container logs: <code>docker logs reddit-slim</code> - Wait ~2-3 minutes for initial data extraction - Check initialization status: <code>docker exec reddit-slim cat /run/postmill.env</code></p> <p>Database reset not working: - Ensure archives exist:   <pre><code>docker exec reddit-slim ls -lh /var/backups/pgsql/data.tar.gz\ndocker exec reddit-slim ls -lh /var/backups/images/submission_images.tar.gz\n</code></pre> - Check services status: <code>docker exec reddit-slim supervisorctl status</code> - Verify database integrity: <code>docker exec reddit-slim postmill-init</code> (shows validation output)</p> <p>Patches not applied: 1. Check rate limits removed:    <pre><code>docker exec reddit-slim grep -c '@RateLimit' /var/www/html/src/DataObject/SubmissionData.php\n</code></pre>    Should return <code>0</code></p> <ol> <li>Check HTTP client configured:    <pre><code>docker exec reddit-slim grep 'alias: postmill.http_client.default' \\\n  /var/www/html/config/packages/http_client.yaml\n</code></pre>    Should show the configuration line</li> </ol>"},{"location":"getting_started/hard_subset/","title":"Hard Subset","text":""},{"location":"getting_started/hard_subset/#overview","title":"Overview","text":"<p>WebArena-Verified Hard is a carefully curated subset of 258 challenging tasks selected from the full 812-task benchmark. This subset focuses on genuinely difficult tasks while maintaining broad site coverage and category diversity.</p> <p>Why use the hard subset?</p> <ul> <li>Cost-effective evaluation: Evaluate on 258 tasks instead of 812 while preserving discriminative power</li> <li>Difficulty-prioritized: 48.1% of tasks have predicted success rate \u2264 0.20</li> <li>Representative coverage: Maintains balanced distribution across sites and task categories</li> </ul>"},{"location":"getting_started/hard_subset/#task-selection","title":"Task Selection","text":"<p>The subset contains:</p> Site Tasks Multi-site Total Shopping Admin 55 - 55 GitLab 57 - 57 Reddit 42 - 42 Shopping 56 - 56 Multi-site - 48 48 Overall 210 48 258 <p>Why no single-site Map tasks?</p> <p>Single-site Map tasks are excluded from the hard subset due to contamination issues identified during benchmark diagnosis. All 48 multi-site tasks (including 19 that involve Map) are included.</p>"},{"location":"getting_started/hard_subset/#how-it-was-created","title":"How It Was Created","text":"<p>The subset was constructed using a principled difficulty modeling approach:</p> <ol> <li>Difficulty Quantification: Estimated task hardness from multi-agent trajectories (8 agents) using a survival-style GLMM that models success probability as a function of steps taken</li> <li>Task Ranking: Ranked tasks by difficulty coefficient (\u03b2_t), where larger values indicate harder tasks</li> <li>Category Balancing: Within each per-site category, selected up to \u03ba tasks based on hardness probability:<ul> <li>Default cap: \u03ba_default = 3 tasks per category</li> <li>Easy category cap: \u03ba_easy = 2 tasks (for categories with median success \u2265 0.85)</li> </ul> </li> <li>Site Coverage: Single-site Map excluded due to contamination; all 48 multi-site tasks included</li> </ol> <p>Selection criteria:</p> <ul> <li>\u03c4_hard = 0.20 (threshold for \"hard\" classification)</li> <li>\u03c4_easy = 0.85 (threshold for \"easy\" category identification)</li> <li>16.7% of tasks have \u2265 0.90 probability of being hard</li> </ul> <p>The hardest categories involve multi-step state-changing interactions (forms, data updates), while easiest are browse/read-only tasks.</p>"},{"location":"getting_started/hard_subset/#usage","title":"Usage","text":"<p>Export the hard subset tasks to a JSON file:</p> uvxDockerCLI <pre><code>uvx webarena-verified subset-export \\\n  --name webarena-verified-hard \\\n  --output webarena-verified-hard.json\n</code></pre> <pre><code>docker run --rm \\\n  -v ./:/output \\\n  am1n3e/webarena-verified:latest \\\n  subset-export \\\n    --name webarena-verified-hard \\\n    --output /output/webarena-verified-hard.json\n</code></pre> <pre><code>webarena-verified subset-export \\\n  --name webarena-verified-hard \\\n  --output webarena-verified-hard.json\n</code></pre> <p>The exported file contains the full task definitions for all 258 tasks in the subset.</p> <p>For more subset management commands, see the Subset Manager guide.</p>"},{"location":"getting_started/hard_subset/#reference","title":"Reference","text":"<p>For detailed methodology and analysis, see Section 4.5 \"WebArena Verified Hard: A Representative Subset\" in the WebArena Verified paper.</p>"},{"location":"getting_started/subset_manager/","title":"Subset Manager","text":"<p>WebArena-Verified provides commands to manage task subsets - curated collections of tasks for focused evaluation.</p>"},{"location":"getting_started/subset_manager/#list-available-subsets","title":"List Available Subsets","text":"<p>View all predefined subsets in the repository:</p> uvxDockerCLI <pre><code>uvx webarena-verified subsets-ls\n</code></pre> <pre><code>docker run --rm am1n3e/webarena-verified:latest subsets-ls\n</code></pre> <pre><code>webarena-verified subsets-ls\n</code></pre> <p>This lists all subset files in <code>assets/dataset/subsets/</code>, including:</p> <ul> <li><code>webarena-verified-hard</code> - Difficulty-prioritized 258-task subset (see Hard Subset)</li> </ul>"},{"location":"getting_started/subset_manager/#export-a-subset","title":"Export a Subset","text":"<p>Export task definitions from a subset to a standalone JSON file:</p> uvxDockerCLI <pre><code># By name\nuvx webarena-verified subset-export \\\n  --name webarena-verified-hard \\\n  --output webarena-verified-hard.json\n\n# By path\nuvx webarena-verified subset-export \\\n  --path assets/dataset/subsets/custom-subset.json \\\n  --output custom-tasks.json\n</code></pre> <pre><code># By name\ndocker run --rm \\\n  -v ./:/output \\\n  am1n3e/webarena-verified:latest \\\n  subset-export \\\n    --name webarena-verified-hard \\\n    --output /output/webarena-verified-hard.json\n\n# By path\ndocker run --rm \\\n  -v ./:/data \\\n  am1n3e/webarena-verified:latest \\\n  subset-export \\\n    --path /data/assets/dataset/subsets/custom-subset.json \\\n    --output /data/custom-tasks.json\n</code></pre> <pre><code># By name\nwebarena-verified subset-export \\\n  --name webarena-verified-hard \\\n  --output webarena-verified-hard.json\n\n# By path\nwebarena-verified subset-export \\\n  --path assets/dataset/subsets/custom-subset.json \\\n  --output custom-tasks.json\n</code></pre> <p>The exported file contains complete task definitions for all tasks in the subset, which you can use for evaluation or analysis.</p>"},{"location":"getting_started/subset_manager/#create-a-custom-subset","title":"Create a Custom Subset","text":"<p>Create a new subset from a custom task list:</p> uvxDockerCLI <pre><code>uvx webarena-verified subsets-create \\\n  --src custom_tasks.json \\\n  --name my-subset \\\n  --desc \"My custom task selection\"\n</code></pre> <pre><code>docker run --rm \\\n  -v ./:/data \\\n  am1n3e/webarena-verified:latest \\\n  subsets-create \\\n    --src /data/custom_tasks.json \\\n    --name my-subset \\\n    --desc \"My custom task selection\"\n</code></pre> <pre><code>webarena-verified subsets-create \\\n  --src custom_tasks.json \\\n  --name my-subset \\\n  --desc \"My custom task selection\"\n</code></pre> <p>This creates a new subset file in <code>assets/dataset/subsets/</code> with:</p> <ul> <li>Task IDs extracted from the source file</li> <li>Optional description</li> <li>Computed checksum for integrity verification</li> </ul>"},{"location":"getting_started/usage/","title":"Usage","text":"<p>This guide walks you through using WebArena-Verified to evaluate web agents. You'll learn how to get task data, run your agent, and evaluate the results using either the CLI or programmatic API.</p>"},{"location":"getting_started/usage/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker or Python 3.11+ (Python only required when installing as a library)</li> <li>WebArena-Verified installed (see Installation)</li> <li>Configuration file set up (see Configuration)</li> </ul>"},{"location":"getting_started/usage/#installation","title":"Installation","text":"uvxDockeruvpip <p>Prerequisites: uv</p> <p>What is uvx?</p> <p><code>uvx</code> runs Python CLI tools in isolated, ephemeral environments without installation. It doesn't pollute your environment and automatically handles dependencies and cleanup.</p> <p>No installation needed! Verify the CLI is working:</p> <pre><code>uvx webarena-verified --help\n</code></pre> <p>Pull the Docker image:</p> <pre><code>docker pull am1n3e/webarena-verified:latest\n</code></pre> <p>Verify the installation:</p> <pre><code>docker run --rm am1n3e/webarena-verified:latest --help\n</code></pre> <pre><code>uv pip install webarena-verified\n</code></pre> <p>Verify the installation:</p> <pre><code>webarena-verified --help\n</code></pre> <pre><code>pip install webarena-verified\n</code></pre> <p>Verify the installation:</p> <pre><code>webarena-verified --help\n</code></pre> <p>For development or contributing, see the Contributing Guide.</p>"},{"location":"getting_started/usage/#step-1-set-up-your-configuration","title":"Step 1: Set Up Your Configuration","text":"<p>Create a configuration file that specifies your environment URLs and credentials:</p> <pre><code>{\n  \"environments\": {\n    \"__GITLAB__\": {\n      \"urls\": [\"http://localhost:8012\"],\n      \"credentials\": {\"username\": \"root\", \"password\": \"demopass\"}\n    },\n    \"__SHOPPING__\": {\n      \"urls\": [\"http://localhost:7770\"]\n    }\n  }\n}\n</code></pre> <p>See Configuration for complete details on all configuration options.</p>"},{"location":"getting_started/usage/#step-2-get-task-data","title":"Step 2: Get Task Data","text":"<p>Export task information that your agent needs using the <code>agent-input-get</code> command:</p> All tasksSpecific tasksFilter by site <pre><code>webarena-verified agent-input-get \\\n  --config config.json \\\n  --output tasks.json\n</code></pre> <pre><code>webarena-verified agent-input-get \\\n  --task-ids 1,2,3 \\\n  --config config.json \\\n  --output tasks.json\n</code></pre> <pre><code>webarena-verified agent-input-get \\\n  --sites shopping \\\n  --config config.json \\\n  --output tasks.json\n</code></pre> <p>The output file contains task metadata your agent needs:</p> <pre><code>[\n  {\n    \"task_id\": 1,\n    \"intent_template_id\": 100,\n    \"sites\": [\"shopping\"],\n    \"start_urls\": [\"http://localhost:7770/...\"],\n    \"intent\": \"What is the price of...\"\n  }\n]\n</code></pre> <p>URL Rendering</p> <p>The <code>--config</code> flag is required to render template URLs (like <code>__SHOPPING__</code>) into actual URLs that your agent can navigate to.</p>"},{"location":"getting_started/usage/#step-3-run-your-agent","title":"Step 3: Run Your Agent","text":"<p>Your agent should:</p> <ol> <li>Load task data from the JSON file produced in Step 2</li> <li>For each task:<ul> <li>Navigate to the provided <code>start_urls</code></li> <li>Execute the task based on the <code>intent</code></li> <li>Save outputs to the expected location</li> </ul> </li> </ol> <p>Required output files per task:</p> <pre><code>{output_dir}/\n\u2514\u2500\u2500 {task_id}/\n    \u251c\u2500\u2500 agent_response.json  # Agent's response (see format below)\n    \u2514\u2500\u2500 network.har          # Network trace in HAR format\n</code></pre> <p>Agent response format:</p> <pre><code>{\n  \"task_type\": \"RETRIEVE\",\n  \"status\": \"SUCCESS\",\n  \"retrieved_data\": [\"extracted data here\"],\n  \"error_details\": null\n}\n</code></pre> Field Type Description <code>task_type</code> string One of: <code>RETRIEVE</code>, <code>MUTATE</code>, <code>NAVIGATE</code> <code>status</code> string One of: <code>SUCCESS</code>, <code>ACTION_NOT_ALLOWED_ERROR</code>, <code>PERMISSION_DENIED_ERROR</code>, <code>NOT_FOUND_ERROR</code>, <code>DATA_VALIDATION_ERROR</code>, <code>UNKNOWN_ERROR</code> <code>retrieved_data</code> array or null Required for <code>RETRIEVE</code> tasks; list of extracted values <code>error_details</code> string or null Optional error description <p>Reference Implementation</p> <p>See the human agent example in <code>examples/agents/human/</code> for a complete reference implementation that demonstrates loading task data, browser automation with Playwright, and producing properly formatted output files.</p>"},{"location":"getting_started/usage/#step-4-evaluate-results","title":"Step 4: Evaluate Results","text":"<p>Use the <code>eval-tasks</code> command to score your agent's outputs:</p>"},{"location":"getting_started/usage/#basic-evaluation","title":"Basic Evaluation","text":"<p>Score one or more runs. When no filters are provided, the CLI discovers every task directory under <code>--output-dir</code> that contains the required files.</p> uvxDockerCLI <pre><code>uvx webarena-verified eval-tasks --output-dir output\n</code></pre> <pre><code>docker run --rm \\\n  -v /path/to/output:/data \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks --output-dir /data\n</code></pre> <pre><code>webarena-verified eval-tasks --output-dir output\n</code></pre>"},{"location":"getting_started/usage/#filtering-tasks","title":"Filtering Tasks","text":"<p>You can filter which tasks to evaluate:</p> uvxDockerCLI <pre><code># Specific task IDs\nuvx webarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --task-ids 1,2,3\n\n# Single task\nuvx webarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --task-ids 42\n\n# By site\nuvx webarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --sites shopping\n\n# By task type\nuvx webarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --task-type mutate\n\n# By template ID\nuvx webarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --template-id 5\n\n# Combined filters\nuvx webarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --sites shopping,reddit \\\n  --task-type mutate\n\n# Dry run (no scoring)\nuvx webarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --sites reddit \\\n  --dry-run\n</code></pre> <pre><code># Specific task IDs\ndocker run --rm \\\n  -v /path/to/output:/data \\\n  -v /path/to/config.json:/config.json \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks --config /config.json --output-dir /data --task-ids 1,2,3\n\n# Single task\ndocker run --rm \\\n  -v /path/to/output:/data \\\n  -v /path/to/config.json:/config.json \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks --config /config.json --output-dir /data --task-ids 42\n\n# By site\ndocker run --rm \\\n  -v /path/to/output:/data \\\n  -v /path/to/config.json:/config.json \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks --config /config.json --output-dir /data --sites shopping\n\n# By task type\ndocker run --rm \\\n  -v /path/to/output:/data \\\n  -v /path/to/config.json:/config.json \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks --config /config.json --output-dir /data --task-type mutate\n\n# By template ID\ndocker run --rm \\\n  -v /path/to/output:/data \\\n  -v /path/to/config.json:/config.json \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks --config /config.json --output-dir /data --template-id 5\n\n# Combined filters\ndocker run --rm \\\n  -v /path/to/output:/data \\\n  -v /path/to/config.json:/config.json \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks --config /config.json --output-dir /data --sites shopping,reddit --task-type mutate\n\n# Dry run (no scoring)\ndocker run --rm \\\n  -v /path/to/output:/data \\\n  -v /path/to/config.json:/config.json \\\n  am1n3e/webarena-verified:latest \\\n  eval-tasks --config /config.json --output-dir /data --sites reddit --dry-run\n</code></pre> <pre><code># Specific task IDs\nwebarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --task-ids 1,2,3\n\n# Single task\nwebarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --task-ids 42\n\n# By site\nwebarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --sites shopping\n\n# By task type\nwebarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --task-type mutate\n\n# By template ID\nwebarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --template-id 5\n\n# Combined filters\nwebarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --sites shopping,reddit \\\n  --task-type mutate\n\n# Dry run (no scoring)\nwebarena-verified eval-tasks \\\n  --config config.json \\\n  --output-dir output \\\n  --sites reddit \\\n  --dry-run\n</code></pre> <p>Available filter flags:</p> Flag Description <code>--task-ids</code> Comma-separated task IDs (for example <code>1,2,3</code> or single <code>42</code>). <code>--sites</code> Comma-separated site names (<code>shopping</code>, <code>reddit</code>, <code>gitlab</code>, <code>map</code>, etc.). <code>--task-type</code> Task type (<code>retrieve</code>, <code>mutate</code>, or <code>navigate</code>). <code>--template-id</code> Filter by <code>intent_template_id</code>. <code>--dry-run</code> List matching tasks without scoring them."},{"location":"getting_started/usage/#understanding-evaluation-output","title":"Understanding Evaluation Output","text":"<p>The CLI writes evaluation artifacts alongside your agent outputs:</p> <pre><code>output/\n\u251c\u2500\u2500 {task_id}/\n\u2502   \u251c\u2500\u2500 agent_response.json  # Agent response produced by the agent\n\u2502   \u251c\u2500\u2500 network.har          # Network trace captured during the run (HAR format)\n\u2502   \u2514\u2500\u2500 eval_result.json     # Evaluation result written by the CLI\n\u2514\u2500\u2500 eval_log_{timestamp}.txt # Batch evaluation log\n</code></pre> <p>See Evaluation Results for details on the evaluation output format.</p>"},{"location":"getting_started/usage/#using-the-programmatic-api","title":"Using the Programmatic API","text":"<p>If you prefer to integrate WebArena-Verified directly into your Python code, you can use the programmatic API.</p>"},{"location":"getting_started/usage/#step-1-initialize-webarenaverified","title":"Step 1: Initialize WebArenaVerified","text":"<p>Create a <code>WebArenaVerified</code> instance with your environment configuration:</p> <pre><code>from pathlib import Path\nfrom webarena_verified.api import WebArenaVerified\nfrom webarena_verified.types.config import WebArenaVerifiedConfig\n\n# Initialize with configuration\nconfig = WebArenaVerifiedConfig(\n    environments={\n        \"__GITLAB__\": {\n            \"urls\": [\"http://localhost:8012\"],\n            \"credentials\": {\"username\": \"root\", \"password\": \"demopass\"}\n        }\n    }\n)\nwa = WebArenaVerified(config=config)\n</code></pre>"},{"location":"getting_started/usage/#step-2-get-task-data_1","title":"Step 2: Get Task Data","text":"<p>Retrieve task information programmatically:</p> <pre><code># Get a single task\ntask = wa.get_task(42)\nprint(f\"Task intent: {task.intent}\")\nprint(f\"Start URLs: {task.start_urls}\")\n\n# Get multiple tasks\ntasks = [wa.get_task(task_id) for task_id in [1, 2, 3]]\n</code></pre>"},{"location":"getting_started/usage/#step-3-evaluate-agent-output","title":"Step 3: Evaluate Agent Output","text":"<p>Once you have your agent's output, evaluate it against the task definition. You can pass agent responses as file paths or construct them directly in code:</p> With FilesWith Content <pre><code># Evaluate a task with file paths\nresult = wa.evaluate_task(\n    task_id=44,\n    agent_response=Path(\"output/44/agent_response.json\"),\n    network_trace=Path(\"output/44/network.har\")\n)\n\nprint(f\"Score: {result.score}, Status: {result.status}\")\n</code></pre> <pre><code>import json\n\n# Evaluate a task with direct content\nresult = wa.evaluate_task(\n    task_id=44,\n    agent_response={\n        \"task_type\": \"NAVIGATE\",\n        \"status\": \"SUCCESS\",\n        \"retrieved_data\": None\n    },\n    network_trace=json.loads(Path(\"output/44/network.har\").read_text())\n)\n\nprint(f\"Score: {result.score}, Status: {result.status}\")\n</code></pre>"},{"location":"getting_started/usage/#see-also","title":"See Also","text":"<ul> <li>Configuration - Complete configuration reference and options</li> <li>Subset Manager - Work with task subsets for focused evaluation</li> </ul>"},{"location":"getting_started/utilities/","title":"Utilities","text":"<p>WebArena-Verified provides several utility commands for managing and optimizing benchmark data.</p>"},{"location":"getting_started/utilities/#trimming-network-logs","title":"Trimming Network Logs","text":"<p>Network log files (HAR format) can become large due to static resources like CSS, JavaScript, images, and fonts. Since these resources are not evaluated (only HTML pages, API requests, and form submissions matter), you can significantly reduce file sizes by removing them.</p> <p>The <code>trim-network-logs</code> command removes entries for skipped resource types while preserving all evaluation-relevant events:</p> uvxDockerCLI <pre><code>uvx webarena-verified trim-network-logs \\\n  --input logs/task_123.har \\\n  --output logs/task_123_trimmed.har\n</code></pre> <pre><code>docker run --rm \\\n  -v ./logs:/logs \\\n  am1n3e/webarena-verified:latest \\\n  trim-network-logs \\\n    --input /logs/task_123.har \\\n    --output /logs/task_123_trimmed.har\n</code></pre> <pre><code>webarena-verified trim-network-logs \\\n  --input logs/task_123.har \\\n  --output logs/task_123_trimmed.har\n</code></pre>"},{"location":"getting_started/utilities/#what-gets-removed","title":"What Gets Removed","text":"<p>The utility uses the same logic as <code>NetworkEvent.is_evaluation_event</code> to identify static resources:</p> <ul> <li>CSS files (<code>.css</code>)</li> <li>JavaScript files (<code>.js</code>)</li> <li>Images (<code>.png</code>, <code>.jpg</code>, <code>.jpeg</code>, <code>.gif</code>, <code>.svg</code>, <code>.webp</code>, <code>.ico</code>)</li> <li>Fonts (<code>.woff</code>, <code>.woff2</code>, <code>.ttf</code>, <code>.eot</code>)</li> </ul>"},{"location":"getting_started/utilities/#what-gets-kept","title":"What Gets Kept","text":"<p>All evaluation-relevant network events are preserved:</p> <ul> <li>HTML pages</li> <li>API endpoints</li> <li>Form submissions</li> <li>All other navigation and data requests</li> </ul>"},{"location":"getting_started/utilities/#benefits","title":"Benefits","text":"<ul> <li>76-90% file size reduction on typical logs</li> <li>Evaluation results unchanged - trimmed files produce identical scores</li> <li>Faster processing - smaller files load and parse more quickly</li> <li>Reduced storage costs - especially important for large-scale evaluations</li> </ul> <p>Example Size Reduction</p> <pre><code># Original HAR file: 786 KB with 359 entries\n# Trimmed HAR file: 184 KB with 50 entries (76.6% reduction)\n</code></pre>"},{"location":"getting_started/utilities/#batch-trimming","title":"Batch Trimming","text":"<p>You can trim multiple files in a loop:</p> uvxDockerCLI <pre><code>for task_dir in output/*/; do\n  task_id=$(basename \"$task_dir\")\n  uvx webarena-verified trim-network-logs \\\n    --input \"$task_dir/network.har\" \\\n    --output \"$task_dir/network_trimmed.har\"\ndone\n</code></pre> <pre><code>for task_dir in output/*/; do\n  task_id=$(basename \"$task_dir\")\n  docker run --rm \\\n    -v ./\"$task_dir\":/data \\\n    am1n3e/webarena-verified:latest \\\n    trim-network-logs \\\n      --input /data/network.har \\\n      --output /data/network_trimmed.har\ndone\n</code></pre> <pre><code>for task_dir in output/*/; do\n  task_id=$(basename \"$task_dir\")\n  webarena-verified trim-network-logs \\\n    --input \"$task_dir/network.har\" \\\n    --output \"$task_dir/network_trimmed.har\"\ndone\n</code></pre>"},{"location":"getting_started/utilities/#technical-details","title":"Technical Details","text":"<p>The trimming utility:</p> <ol> <li>Loads the HAR file and converts entries to <code>NetworkEvent</code> objects</li> <li>Uses <code>NetworkEvent.is_evaluation_event</code> to identify evaluation-relevant events</li> <li>Filters out static resources while preserving evaluation events</li> <li>Writes the trimmed HAR file with the same structure</li> </ol> <p>This ensures consistency with the evaluation logic and guarantees that trimmed logs produce identical evaluation results.</p>"}]}